#!/usr/bin/env python3
"""
API Performance Benchmark –¥–ª—è H3 endpoints
–¢–µ—Å—Ç—É—î —Ä—ñ–∑–Ω—ñ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó metric_type, resolution —Ç–∞ limits
"""

import requests
import time
import json
from typing import Dict, List, Tuple
from dataclasses import dataclass
import pandas as pd

@dataclass
class BenchmarkResult:
    metric_type: str
    resolution: int
    limit: int
    response_time_ms: int
    hexagons_count: int
    response_size_kb: int
    success: bool
    error_message: str = ""

class APIBenchmark:
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.endpoint = f"{base_url}/api/v1/visualization/kyiv-h3"
        self.results: List[BenchmarkResult] = []
    
    def test_single_request(self, metric_type: str, resolution: int, limit: int) -> BenchmarkResult:
        """–¢–µ—Å—Ç—É—î –æ–¥–∏–Ω API –∑–∞–ø–∏—Ç"""
        params = {
            'metric_type': metric_type,
            'resolution': resolution,
            'limit': limit
        }
        
        try:
            print(f"üîÑ Testing {metric_type} H3-{resolution} limit={limit}...")
            
            start_time = time.time()
            response = requests.get(self.endpoint, params=params, timeout=30)
            end_time = time.time()
            
            response_time_ms = int((end_time - start_time) * 1000)
            
            if response.status_code == 200:
                data = response.json()
                hexagons_count = len(data.get('hexagons', []))
                response_size_kb = len(response.content) // 1024
                
                result = BenchmarkResult(
                    metric_type=metric_type,
                    resolution=resolution,
                    limit=limit,
                    response_time_ms=response_time_ms,
                    hexagons_count=hexagons_count,
                    response_size_kb=response_size_kb,
                    success=True
                )
                print(f"‚úÖ {response_time_ms}ms, {hexagons_count} hexagons, {response_size_kb}KB")
                return result
            else:
                print(f"‚ùå HTTP {response.status_code}")
                return BenchmarkResult(
                    metric_type=metric_type,
                    resolution=resolution,
                    limit=limit,
                    response_time_ms=response_time_ms,
                    hexagons_count=0,
                    response_size_kb=0,
                    success=False,
                    error_message=f"HTTP {response.status_code}"
                )
                
        except Exception as e:
            print(f"‚ùå Error: {str(e)}")
            return BenchmarkResult(
                metric_type=metric_type,
                resolution=resolution,
                limit=limit,
                response_time_ms=0,
                hexagons_count=0,
                response_size_kb=0,
                success=False,
                error_message=str(e)
            )
    
    def run_full_benchmark(self):
        """–ó–∞–ø—É—Å–∫–∞—î –ø–æ–≤–Ω–∏–π benchmark –≤—Å—ñ—Ö –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π"""
        
        # –î–∞–Ω—ñ –∑ –±–∞–∑–∏ –ø—Ä–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –≥–µ–∫—Å–∞–≥–æ–Ω—ñ–≤
        db_stats = {
            7: 8714,
            8: 30840, 
            9: 78220,
            10: 141502
        }
        
        print("üöÄ –ó–∞–ø—É—Å–∫–∞—î–º–æ API Performance Benchmark")
        print("=" * 50)
        
        # –¢–µ—Å—Ç–æ–≤—ñ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó
        metrics = ['opportunity', 'competition']
        resolutions = [7, 8, 9, 10]
        
        # –†—ñ–∑–Ω—ñ limits –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è
        test_limits = [
            1000,    # –ú–∞–ª–∏–π dataset
            5000,    # –°–µ—Ä–µ–¥–Ω—ñ–π dataset  
            50000,   # –í–µ–ª–∏–∫–∏–π dataset
            1000000  # Unlimited (—è–∫ –≤ legacy)
        ]
        
        total_tests = len(metrics) * len(resolutions) * len(test_limits)
        current_test = 0
        
        for metric in metrics:
            for resolution in resolutions:
                expected_hexagons = db_stats[resolution]
                
                for limit in test_limits:
                    current_test += 1
                    print(f"\n[{current_test}/{total_tests}] ", end="")
                    
                    # –î–æ–¥–∞—î–º–æ –Ω–µ–≤–µ–ª–∏–∫—É –ø–∞—É–∑—É –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏
                    if current_test > 1:
                        time.sleep(0.5)
                    
                    result = self.test_single_request(metric, resolution, limit)
                    self.results.append(result)
        
        print(f"\n‚úÖ Benchmark –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –ü—Ä–æ—Ç–µ—Å—Ç–æ–≤–∞–Ω–æ {total_tests} –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π")
        
    def analyze_results(self):
        """–ê–Ω–∞–ª—ñ–∑—É—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ benchmark"""
        if not self.results:
            print("‚ùå –ù–µ–º–∞—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É")
            return
            
        print("\n" + "=" * 80)
        print("üìä –ê–ù–ê–õ–Ü–ó –†–ï–ó–£–õ–¨–¢–ê–¢–Ü–í BENCHMARK")
        print("=" * 80)
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ DataFrame –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
        df_data = []
        for result in self.results:
            if result.success:
                df_data.append({
                    'metric': result.metric_type,
                    'resolution': result.resolution,
                    'limit': result.limit,
                    'time_ms': result.response_time_ms,
                    'hexagons': result.hexagons_count,
                    'size_kb': result.response_size_kb,
                    'time_per_hexagon': result.response_time_ms / max(result.hexagons_count, 1)
                })
        
        if not df_data:
            print("‚ùå –ù–µ–º–∞—î —É—Å–ø—ñ—à–Ω–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É")
            return
            
        df = pd.DataFrame(df_data)
        
        # –¢–∞–±–ª–∏—Ü—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        print("\nüîç –î–ï–¢–ê–õ–¨–ù–Ü –†–ï–ó–£–õ–¨–¢–ê–¢–ò:")
        print(f"{'Metric':<12} {'Res':<4} {'Limit':<8} {'Time(ms)':<10} {'Hexagons':<10} {'Size(KB)':<10} {'ms/hex':<8}")
        print("-" * 75)
        
        for _, row in df.iterrows():
            print(f"{row['metric']:<12} H3-{row['resolution']:<3} {row['limit']:<8} {row['time_ms']:<10} {row['hexagons']:<10} {row['size_kb']:<10} {row['time_per_hexagon']:.2f}")
        
        # –ê–Ω–∞–ª—ñ–∑ –ø–æ resolutions
        print(f"\nüìä –°–ï–†–ï–î–ù–Ü–ô –ß–ê–° –ü–û RESOLUTIONS:")
        res_analysis = df.groupby('resolution').agg({
            'time_ms': ['mean', 'min', 'max'],
            'hexagons': 'mean',
            'size_kb': 'mean'
        }).round(2)
        print(res_analysis)
        
        # Legacy simulation (8 –∑–∞–ø–∏—Ç—ñ–≤ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ)
        print(f"\n‚ö° LEGACY vs SMART LOADING –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø:")
        
        # Legacy: –≤—Å—ñ 8 –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ –∑ limit=1000000
        legacy_total = 0
        legacy_requests = []
        
        for metric in ['opportunity', 'competition']:
            for res in [7, 8, 9, 10]:
                matching = df[(df['metric'] == metric) & 
                             (df['resolution'] == res) & 
                             (df['limit'] == 1000000)]
                if not matching.empty:
                    time_ms = matching.iloc[0]['time_ms']
                    legacy_total += time_ms
                    legacy_requests.append(f"{metric} H3-{res}: {time_ms}ms")
        
        print(f"\n‚ùå LEGACY APPROACH (–ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ):")
        for req in legacy_requests:
            print(f"  {req}")
        print(f"  –ó–ê–ì–ê–õ–û–ú: {legacy_total}ms = {legacy_total/1000:.1f} —Å–µ–∫—É–Ω–¥")
        
        # Smart Loading simulation
        print(f"\n‚úÖ SMART LOADING APPROACH:")
        
        # Tier 1: opportunity H3-8 limit=1000
        tier1 = df[(df['metric'] == 'opportunity') & 
                   (df['resolution'] == 8) & 
                   (df['limit'] == 1000)]
        
        if not tier1.empty:
            tier1_time = tier1.iloc[0]['time_ms']
            print(f"  Tier 1 (critical): {tier1_time}ms - opportunity H3-8")
            print(f"  üìà User can interact after {tier1_time/1000:.1f} seconds!")
            
            improvement = ((legacy_total - tier1_time) / legacy_total) * 100
            print(f"  üöÄ Improvement: {improvement:.1f}% faster time to interactive!")
        
        # –í–∏—è–≤–ª–µ–Ω–Ω—è bottlenecks
        print(f"\nüéØ –í–£–ó–¨–ö–Ü –ú–Ü–°–¶–Ø:")
        slowest = df.nlargest(3, 'time_ms')
        for _, row in slowest.iterrows():
            print(f"  {row['metric']} H3-{row['resolution']} limit={row['limit']}: {row['time_ms']}ms")
            
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó
        print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–Ü–á –î–õ–Ø SMART LOADING:")
        
        # –ù–∞–π—à–≤–∏–¥—à—ñ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó –¥–ª—è Tier 1
        fastest_small = df[df['limit'] <= 1000].nsmallest(3, 'time_ms')
        print(f"  Tier 1 candidates (—à–≤–∏–¥–∫—ñ + –º–∞–ª—ñ):")
        for _, row in fastest_small.iterrows():
            print(f"    {row['metric']} H3-{row['resolution']}: {row['time_ms']}ms, {row['hexagons']} hex")
            
        print(f"\nüéØ –í–ò–°–ù–û–í–ö–ò:")
        print(f"  ‚Ä¢ Legacy –±–ª–æ–∫—É—î UI –Ω–∞ {legacy_total/1000:.1f} —Å–µ–∫—É–Ω–¥")
        print(f"  ‚Ä¢ Smart Tier 1 –¥–∞—î –≤–∑–∞—î–º–æ–¥—ñ—é —á–µ—Ä–µ–∑ {tier1_time/1000:.1f} —Å–µ–∫—É–Ω–¥–∏")
        print(f"  ‚Ä¢ –ü–æ—Ç–µ–Ω—Ü—ñ–∞–ª –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è: {improvement:.0f}%")

def main():
    benchmark = APIBenchmark()
    
    print("üß™ H3 API Performance Benchmark")
    print("–¢–µ—Å—Ç—É—î–º–æ —Ä—ñ–∑–Ω—ñ combinations metric_type + resolution + limit")
    print()
    
    # –ó–∞–ø—É—Å–∫–∞—î–º–æ benchmark
    benchmark.run_full_benchmark()
    
    # –ê–Ω–∞–ª—ñ–∑—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
    benchmark.analyze_results()
    
    print(f"\nüéØ Benchmark –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

if __name__ == "__main__":
    main()