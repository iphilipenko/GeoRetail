#!/usr/bin/env python3
"""
Find Brand Candidates Script
–ê–Ω–∞–ª—ñ–∑—É—î poi_processed –¥–ª—è –∑–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è POI –±–µ–∑ —Ä–æ–∑–ø—ñ–∑–Ω–∞–Ω–∏—Ö –±—Ä–µ–Ω–¥—ñ–≤
—Ç–∞ —Å—Ç–≤–æ—Ä—é—î –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤ –¥–ª—è –ø–æ–¥–∞–ª—å—à–æ–≥–æ review —Ç–∞ –∑–∞—Ç–≤–µ—Ä–¥–∂–µ–Ω–Ω—è
"""

import sys
import logging
import psycopg2
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from psycopg2.extras import RealDictCursor
import json

# –î–æ–¥–∞—î–º–æ –ø–æ—Ç–æ—á–Ω—É –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é –¥–æ path –¥–ª—è —ñ–º–ø–æ—Ä—Ç—ñ–≤
sys.path.insert(0, str(Path(__file__).parent.parent))

# –Ü–º–ø–æ—Ä—Ç–∏ –Ω–∞—à–∏—Ö –º–æ–¥—É–ª—ñ–≤
from normalization.brand_manager import BrandManager
from normalization.brand_matcher import BrandMatcher

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è (hardcoded –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ)
CONFIG = {
    'analysis': {
        'min_frequency': 3,              # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ —á–∞—Å—Ç–æ—Ç–∞ –¥–ª—è —Ä–æ–∑–≥–ª—è–¥—É
        'min_quality_score': 0.0,       # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ —è–∫—ñ—Å—Ç—å POI
        'min_confidence': 0.3,           # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è  
        'min_regions_for_network': 2,   # –ú—ñ–Ω—ñ–º—É–º —Ä–µ–≥—ñ–æ–Ω—ñ–≤ –¥–ª—è –º–µ—Ä–µ–∂—ñ
        'min_name_length': 3,            # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ –Ω–∞–∑–≤–∏
        'max_name_length': 50,           # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ –Ω–∞–∑–≤–∏
    },
    'performance': {
        'batch_size': 1000,              # –†–æ–∑–º—ñ—Ä batch –¥–ª—è –æ–±—Ä–æ–±–∫–∏
        'regions_parallel': False,       # –ü–æ–∫–∏ –±–µ–∑ –ø–∞—Ä–∞–ª–µ–ª—ñ–∑–º—É
    }
}

# Generic names patterns - –±–∞–∑–æ–≤–∏–π —Ñ—ñ–ª—å—Ç—Ä
GENERIC_PATTERNS = [
    '–º–∞–≥–∞–∑–∏–Ω', '–º–∞–≥–∞–∑—ñ–Ω', '–∞–ø—Ç–µ–∫–∞', '–∫–∞—Ñ–µ', '—Ä–µ—Å—Ç–æ—Ä–∞–Ω', '–±–∞–Ω–∫',
    'shop', 'store', 'cafe', 'restaurant', 'pharmacy', 'market',
    '—Å—É–ø–µ—Ä–º–∞—Ä–∫–µ—Ç', '–º—ñ–Ω—ñ–º–∞—Ä–∫–µ—Ç', '–≥–∞—Å—Ç—Ä–æ–Ω–æ–º', '–ø—Ä–æ–¥—É–∫—Ç–∏'
]

# Database connection
DB_CONNECTION_STRING = "postgresql://georetail_user:georetail_secure_2024@localhost:5432/georetail"


class BrandCandidateFinder:
    """–û—Å–Ω–æ–≤–Ω–∏–π –∫–ª–∞—Å –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤ –±—Ä–µ–Ω–¥—ñ–≤"""
    
    def __init__(self, db_connection_string: str):
        self.db_connection_string = db_connection_string
        self.brand_manager = BrandManager(db_connection_string)
        self.brand_matcher = BrandMatcher()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'regions_processed': 0,
            'poi_analyzed': 0,
            'unique_names_found': 0,
            'generic_filtered': 0,
            'existing_brand_matches': 0,
            'quality_candidates': 0,
            'network_candidates': 0,
            'saved_candidates': 0,
            'errors': 0
        }
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∞–Ω–∞–ª—ñ–∑—É
        self.regional_results = {}  # {region_name: [candidates]}
        self.aggregated_candidates = {}  # {name: aggregated_data}
        
        logger.info("‚úÖ BrandCandidateFinder —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ")
    
    def run_analysis(self) -> Dict[str, Any]:
        """–ì–æ–ª–æ–≤–Ω–∏–π –º–µ—Ç–æ–¥ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –∞–Ω–∞–ª—ñ–∑—É"""
        logger.info("üîç –ü–æ—á–∏–Ω–∞—î–º–æ –∞–Ω–∞–ª—ñ–∑ –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤ –±—Ä–µ–Ω–¥—ñ–≤...")
        
        start_time = datetime.now()
        
        try:
            # 1. –í–∞–ª—ñ–¥–∞—Ü—ñ—è
            self._validate_prerequisites()
            
            # 2. –û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å–ø–∏—Å–∫—É —Ä–µ–≥—ñ–æ–Ω—ñ–≤
            regions = self._get_available_regions()
            logger.info(f"üìç –ó–Ω–∞–π–¥–µ–Ω–æ {len(regions)} —Ä–µ–≥—ñ–æ–Ω—ñ–≤ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É")
            
            # 3. –ê–Ω–∞–ª—ñ–∑ –ø–æ —Ä–µ–≥—ñ–æ–Ω–∞—Ö
            for region in regions:
                try:
                    self._analyze_single_region(region)
                    self.stats['regions_processed'] += 1
                except Exception as e:
                    logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ —Ä–µ–≥—ñ–æ–Ω—É {region}: {e}")
                    self.stats['errors'] += 1
                    continue
            
            # 4. –ê–≥—Ä–µ–≥–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö –ø–æ —Ä–µ–≥—ñ–æ–Ω–∞—Ö
            self._aggregate_regional_data()
            
            # 5. –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –º–µ—Ä–µ–∂–µ–≤–æ–≥–æ –ø–æ—Ç–µ–Ω—Ü—ñ–∞–ª—É
            self._calculate_network_potential()
            
            # 6. –ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è —Ñ—ñ–Ω–∞–ª—å–Ω–∏—Ö —Ñ—ñ–ª—å—Ç—Ä—ñ–≤
            final_candidates = self._apply_quality_filters()
            
            # 7. –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤
            saved_count = self._save_candidates(final_candidates)
            self.stats['saved_candidates'] = saved_count
            
            # 8. –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∑–≤—ñ—Ç—É
            execution_time = datetime.now() - start_time
            report = self._generate_summary_report(execution_time)
            
            logger.info("‚úÖ –ê–Ω–∞–ª—ñ–∑ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ!")
            return report
            
        except Exception as e:
            logger.error(f"üí• –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –∞–Ω–∞–ª—ñ–∑—É: {e}")
            raise
    
    def _validate_prerequisites(self):
        """–í–∞–ª—ñ–¥–∞—Ü—ñ—è –ø–µ—Ä–µ–¥—É–º–æ–≤ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É"""
        logger.info("üîß –í–∞–ª—ñ–¥–∞—Ü—ñ—è –ø–µ—Ä–µ–¥—É–º–æ–≤...")
        
        try:
            with psycopg2.connect(self.db_connection_string) as conn:
                with conn.cursor() as cur:
                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —ñ—Å–Ω—É–≤–∞–Ω–Ω—è —Ç–∞–±–ª–∏—Ü—å
                    cur.execute("""
                        SELECT table_name 
                        FROM information_schema.tables 
                        WHERE table_schema = 'osm_ukraine' 
                        AND table_name IN ('poi_processed', 'brand_candidates')
                    """)
                    
                    existing_tables = [row[0] for row in cur.fetchall()]
                    
                    if 'poi_processed' not in existing_tables:
                        raise Exception("–¢–∞–±–ª–∏—Ü—è osm_ukraine.poi_processed –Ω–µ —ñ—Å–Ω—É—î")
                    
                    if 'brand_candidates' not in existing_tables:
                        raise Exception("–¢–∞–±–ª–∏—Ü—è osm_ukraine.brand_candidates –Ω–µ —ñ—Å–Ω—É—î")
                    
                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å POI
                    cur.execute("""
                        SELECT COUNT(*) 
                        FROM osm_ukraine.poi_processed 
                        WHERE entity_type = 'poi'
                    """)
                    
                    poi_count = cur.fetchone()[0]
                    if poi_count == 0:
                        raise Exception("–ù–µ–º–∞—î POI –≤ —Ç–∞–±–ª–∏—Ü—ñ poi_processed")
                    
                    logger.info(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ {poi_count:,} POI –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É")
                    
        except psycopg2.Error as e:
            raise Exception(f"–ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ –ë–î: {e}")
    
    def _get_available_regions(self) -> List[str]:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å–ø–∏—Å–∫—É –¥–æ—Å—Ç—É–ø–Ω–∏—Ö —Ä–µ–≥—ñ–æ–Ω—ñ–≤"""
        regions = []
        
        try:
            with psycopg2.connect(self.db_connection_string) as conn:
                with conn.cursor() as cur:
                    cur.execute("""
                        SELECT DISTINCT region_name
                        FROM osm_ukraine.poi_processed 
                        WHERE entity_type = 'poi'
                        AND brand_normalized IS NULL
                        AND quality_score >= %s
                        ORDER BY region_name
                    """, (CONFIG['analysis']['min_quality_score'],))
                    
                    regions = [row[0] for row in cur.fetchall()]
                    
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Ä–µ–≥—ñ–æ–Ω—ñ–≤: {e}")
            raise
        
        return regions
    
    def _analyze_single_region(self, region_name: str):
        """–ê–Ω–∞–ª—ñ–∑ –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤ –≤ –æ–¥–Ω–æ–º—É —Ä–µ–≥—ñ–æ–Ω—ñ"""
        logger.info(f"üîç –ê–Ω–∞–ª—ñ–∑—É—î–º–æ —Ä–µ–≥—ñ–æ–Ω: {region_name}")
        
        try:
            with psycopg2.connect(self.db_connection_string) as conn:
                with conn.cursor(cursor_factory=RealDictCursor) as cur:
                    # SQL –∑–∞–ø–∏—Ç –¥–ª—è –∑–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è POI –±–µ–∑ –±—Ä–µ–Ω–¥—ñ–≤
                    cur.execute("""
                        SELECT 
                            name_original,
                            primary_category,
                            secondary_category,
                            COUNT(*) as frequency,
                            array_agg(DISTINCT h3_res_8) as h3_hexes,
                            AVG(quality_score) as avg_quality,
                            MIN(quality_score) as min_quality,
                            MAX(quality_score) as max_quality,
                            array_agg(DISTINCT secondary_category) as categories
                        FROM osm_ukraine.poi_processed 
                        WHERE region_name = %s
                          AND brand_normalized IS NULL 
                          AND entity_type = 'poi'
                          AND quality_score >= %s
                          AND name_original IS NOT NULL
                          AND length(name_original) BETWEEN %s AND %s
                        GROUP BY name_original, primary_category, secondary_category
                        HAVING COUNT(*) >= %s
                        ORDER BY COUNT(*) DESC
                    """, (
                        region_name,
                        CONFIG['analysis']['min_quality_score'],
                        CONFIG['analysis']['min_name_length'],
                        CONFIG['analysis']['max_name_length'],
                        CONFIG['analysis']['min_frequency']
                    ))
                    
                    regional_candidates = []
                    
                    for row in cur.fetchall():
                        candidate = dict(row)
                        candidate['region_name'] = region_name
                        
                        # –í–∞–ª—ñ–¥–∞—Ü—ñ—è –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
                        if self._validate_candidate(candidate):
                            regional_candidates.append(candidate)
                            self.stats['poi_analyzed'] += candidate['frequency']
                    
                    self.regional_results[region_name] = regional_candidates
                    self.stats['unique_names_found'] += len(regional_candidates)
                    
                    logger.info(f"üìä {region_name}: –∑–Ω–∞–π–¥–µ–Ω–æ {len(regional_candidates)} –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤")
                    
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –∞–Ω–∞–ª—ñ–∑—É —Ä–µ–≥—ñ–æ–Ω—É {region_name}: {e}")
            raise
    
    def _validate_candidate(self, candidate: Dict[str, Any]) -> bool:
        """–í–∞–ª—ñ–¥–∞—Ü—ñ—è –æ–∫—Ä–µ–º–æ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞"""
        name = candidate['name_original'].strip().lower()
        
        # 1. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ generic –Ω–∞–∑–≤–∏
        if self._is_generic_name(name):
            self.stats['generic_filtered'] += 1
            return False
        
        # 2. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —á–µ—Ä–µ–∑ BrandMatcher (—á–∏ –Ω–µ –ø—Ä–æ–ø—É—Å—Ç–∏–ª–∏ —ñ—Å–Ω—É—é—á–∏–π –±—Ä–µ–Ω–¥)
        try:
            brand_result = self.brand_matcher.match_brand(
                candidate['name_original'],
                {'shop': candidate.get('secondary_category', '')}
            )
            
            if brand_result and brand_result.confidence > 0.8:
                logger.debug(f"üîç –ó–Ω–∞–π–¥–µ–Ω–æ —ñ—Å–Ω—É—é—á–∏–π –±—Ä–µ–Ω–¥ –¥–ª—è '{candidate['name_original']}': {brand_result.canonical_name}")
                self.stats['existing_brand_matches'] += 1
                return False
                
        except Exception as e:
            logger.warning(f"–ü–æ–º–∏–ª–∫–∞ BrandMatcher –¥–ª—è '{candidate['name_original']}': {e}")
        
        # 3. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —è–∫–æ—Å—Ç—ñ
        if candidate['avg_quality'] < CONFIG['analysis']['min_quality_score']:
            return False
        
        return True
    
    def _is_generic_name(self, name: str) -> bool:
        """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —á–∏ —î –Ω–∞–∑–≤–∞ –∑–∞–≥–∞–ª—å–Ω–æ—é (generic)"""
        name_lower = name.lower().strip()
        
        # –¢–æ—á–Ω–µ —Å–ø—ñ–≤–ø–∞–¥—ñ–Ω–Ω—è –∑ generic patterns
        if name_lower in GENERIC_PATTERNS:
            return True
        
        # Pattern matching –¥–ª—è —Ç–∏–ø–æ–≤–∏—Ö –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü—ñ–π
        generic_patterns_regex = [
            r'^–º–∞–≥–∞–∑–∏–Ω\s*\d*$',
            r'^–∞–ø—Ç–µ–∫–∞\s*\d*$', 
            r'^–∫–∞—Ñ–µ\s*\d*$',
            r'^\d+$',  # –¢—ñ–ª—å–∫–∏ —Ü–∏—Ñ—Ä–∏
            r'^[–∞-—è]{1,2}\d+$',  # –ö–æ—Ä–æ—Ç–∫—ñ –∞–±—Ä–µ–≤—ñ–∞—Ç—É—Ä–∏ + —Ü–∏—Ñ—Ä–∏
        ]
        
        import re
        for pattern in generic_patterns_regex:
            if re.match(pattern, name_lower):
                return True
        
        return False
    
    def _aggregate_regional_data(self):
        """–ê–≥—Ä–µ–≥–∞—Ü—ñ—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –ø–æ –≤—Å—ñ—Ö —Ä–µ–≥—ñ–æ–Ω–∞—Ö"""
        logger.info("üîÑ –ê–≥—Ä–µ–≥—É—î–º–æ –¥–∞–Ω—ñ –ø–æ —Ä–µ–≥—ñ–æ–Ω–∞—Ö...")
        
        aggregated = {}
        
        for region_name, candidates in self.regional_results.items():
            for candidate in candidates:
                name = candidate['name_original']
                
                if name not in aggregated:
                    aggregated[name] = {
                        'name_original': name,
                        'total_frequency': 0,
                        'regions': [],
                        'categories': set(),
                        'h3_coverage': set(),
                        'quality_scores': [],
                        'primary_categories': set()
                    }
                
                # –ê–≥—Ä–µ–≥—É—î–º–æ –¥–∞–Ω—ñ
                agg_data = aggregated[name]
                agg_data['total_frequency'] += candidate['frequency']
                agg_data['regions'].append(region_name)
                agg_data['categories'].update(candidate['categories'] or [])
                agg_data['h3_coverage'].update(candidate['h3_hexes'] or [])
                agg_data['quality_scores'].append(candidate['avg_quality'])
                agg_data['primary_categories'].add(candidate['primary_category'])
        
        self.aggregated_candidates = aggregated
        logger.info(f"üìä –ê–≥—Ä–µ–≥–æ–≤–∞–Ω–æ {len(aggregated)} —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –Ω–∞–∑–≤")
    
    def _calculate_network_potential(self):
        """–†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –º–µ—Ä–µ–∂–µ–≤–æ–≥–æ –ø–æ—Ç–µ–Ω—Ü—ñ–∞–ª—É –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞"""
        logger.info("üßÆ –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ –º–µ—Ä–µ–∂–µ–≤–∏–π –ø–æ—Ç–µ–Ω—Ü—ñ–∞–ª...")
        
        for name, data in self.aggregated_candidates.items():
            # –û—Å–Ω–æ–≤–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏
            region_count = len(data['regions'])
            total_frequency = data['total_frequency']
            quality_scores = [float(score) for score in data['quality_scores']]
            avg_quality = sum(quality_scores) / len(quality_scores)
            h3_spread = len(data['h3_coverage'])
            
            # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ scores
            network_score = min((region_count - 1) / 5.0, 1.0) if region_count >= 2 else 0.0
            frequency_score = min(total_frequency / 50.0, 1.0)
            geographic_score = min(h3_spread / 20.0, 1.0)
            
            # –ö–æ–º–±—ñ–Ω–æ–≤–∞–Ω–∏–π confidence score
            confidence = (
                network_score * 0.4 +      # –ù–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–µ - –º–µ—Ä–µ–∂–µ–≤—ñ—Å—Ç—å
                frequency_score * 0.3 +    # –ß–∞—Å—Ç–æ—Ç–∞ –ø–æ—è–≤–∏  
                avg_quality * 0.2 +        # –Ø–∫—ñ—Å—Ç—å –¥–∞–Ω–∏—Ö
                geographic_score * 0.1     # –ì–µ–æ–≥—Ä–∞—Ñ—ñ—á–Ω–µ –ø–æ–∫—Ä–∏—Ç—Ç—è
            )
            
            # –î–æ–¥–∞—î–º–æ —Ä–æ–∑—Ä–∞—Ö–æ–≤–∞–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏
            data.update({
                'region_count': region_count,
                'network_score': float(network_score),
                'frequency_score': float(frequency_score),
                'geographic_score': geographic_score,
                'avg_quality': avg_quality,
                'confidence_score': confidence,
                'is_network_candidate': region_count >= CONFIG['analysis']['min_regions_for_network']
            })
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        network_candidates = sum(1 for data in self.aggregated_candidates.values() 
                               if data['is_network_candidate'])
        self.stats['network_candidates'] = network_candidates
        
        logger.info(f"üìà –ó–Ω–∞–π–¥–µ–Ω–æ {network_candidates} –º–µ—Ä–µ–∂–µ–≤–∏—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤")
    
    def _apply_quality_filters(self) -> List[Dict[str, Any]]:
        """–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è —Ñ—ñ–Ω–∞–ª—å–Ω–∏—Ö —Ñ—ñ–ª—å—Ç—Ä—ñ–≤ —è–∫–æ—Å—Ç—ñ"""
        logger.info("üîß –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ —Ñ—ñ–ª—å—Ç—Ä–∏ —è–∫–æ—Å—Ç—ñ...")
        
        quality_candidates = []
        
        for name, data in self.aggregated_candidates.items():
            # –§—ñ–ª—å—Ç—Ä –ø–æ –º—ñ–Ω—ñ–º–∞–ª—å–Ω—ñ–π –≤–ø–µ–≤–Ω–µ–Ω–æ—Å—Ç—ñ
            if data['confidence_score'] >= CONFIG['analysis']['min_confidence']:
                # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è
                candidate = {
                    'name': name,
                    'frequency': data['total_frequency'],
                    'locations': data['regions'],
                    'categories': list(data['categories']),
                    'confidence_score': round(data['confidence_score'], 3),
                    'suggested_canonical_name': name,  # –ü–æ–∫–∏ –±–µ–∑ –æ–±—Ä–æ–±–∫–∏
                    'suggested_functional_group': self._suggest_functional_group(data),
                    'suggested_influence_weight': self._suggest_influence_weight(data),
                    'suggested_format': self._suggest_format(data),
                    'recommendation_reason': self._generate_recommendation_reason(data)
                }
                
                quality_candidates.append(candidate)
        
        self.stats['quality_candidates'] = len(quality_candidates)
        
        # –°–æ—Ä—Ç—É—î–º–æ –ø–æ –≤–ø–µ–≤–Ω–µ–Ω–æ—Å—Ç—ñ
        quality_candidates.sort(key=lambda x: x['confidence_score'], reverse=True)
        
        logger.info(f"‚úÖ –í—ñ–¥—ñ–±—Ä–∞–Ω–æ {len(quality_candidates)} —è–∫—ñ—Å–Ω–∏—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤")
        return quality_candidates
    
    def _suggest_functional_group(self, data: Dict[str, Any]) -> str:
        """–ü—Ä–æ–ø–æ–Ω—É—î–º–æ —Ñ—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª—å–Ω—É –≥—Ä—É–ø—É –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∫–∞—Ç–µ–≥–æ—Ä—ñ–π"""
        primary_cats = data['primary_categories']
        
        if 'retail' in primary_cats:
            return 'competitor'
        elif 'food_service' in primary_cats:
            return 'competitor'
        elif 'transport' in primary_cats:
            return 'accessibility'
        else:
            return 'traffic_generator'
    
    def _suggest_influence_weight(self, data: Dict[str, Any]) -> float:
        """–ü—Ä–æ–ø–æ–Ω—É—î–º–æ –≤–∞–≥—É –≤–ø–ª–∏–≤—É"""
        functional_group = self._suggest_functional_group(data)
        
        if functional_group == 'competitor':
            # –ß–∏–º –±—ñ–ª—å—à–µ –º–µ—Ä–µ–∂–∞, —Ç–∏–º —Å–∏–ª—å–Ω—ñ—à–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü—ñ—è
            return -0.3 - (data['region_count'] * 0.1)
        elif functional_group == 'accessibility':
            return 0.3 + (data['region_count'] * 0.05)
        else:
            return 0.1
    
    def _suggest_format(self, data: Dict[str, Any]) -> str:
        """–ü—Ä–æ–ø–æ–Ω—É—î–º–æ —Ñ–æ—Ä–º–∞—Ç –∑–∞–∫–ª–∞–¥—É"""
        categories = data['categories']
        
        if 'supermarket' in categories or 'convenience' in categories:
            return '–º–∞–≥–∞–∑–∏–Ω'
        elif 'restaurant' in categories or 'cafe' in categories:
            return '–∑–∞–∫–ª–∞–¥ —Ö–∞—Ä—á—É–≤–∞–Ω–Ω—è'
        elif 'pharmacy' in categories:
            return '–∞–ø—Ç–µ–∫–∞'
        else:
            return '–∑–∞–∫–ª–∞–¥ –æ–±—Å–ª—É–≥–æ–≤—É–≤–∞–Ω–Ω—è'
    
    def _generate_recommendation_reason(self, data: Dict[str, Any]) -> str:
        """–ì–µ–Ω–µ—Ä—É—î–º–æ –ø—Ä–∏—á–∏–Ω—É —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó"""
        reasons = []
        
        if data['region_count'] >= 3:
            reasons.append(f"–º–µ—Ä–µ–∂–∞ –≤ {data['region_count']} —Ä–µ–≥—ñ–æ–Ω–∞—Ö")
        elif data['region_count'] == 2:
            reasons.append("–ø—Ä–∏—Å—É—Ç–Ω—ñ—Å—Ç—å –≤ 2 —Ä–µ–≥—ñ–æ–Ω–∞—Ö")
        
        if data['total_frequency'] >= 20:
            reasons.append(f"–≤–∏—Å–æ–∫–∞ —á–∞—Å—Ç–æ—Ç–∞ ({data['total_frequency']} –ª–æ–∫–∞—Ü—ñ–π)")
        
        if data['avg_quality'] >= 0.8:
            reasons.append("–≤–∏—Å–æ–∫–∞ —è–∫—ñ—Å—Ç—å –¥–∞–Ω–∏—Ö")
        
        return f"–ê–≤—Ç–æ-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è: {', '.join(reasons)}"
    
    def _save_candidates(self, candidates: List[Dict[str, Any]]) -> int:
        """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤ –≤ –ë–î"""
        if not candidates:
            logger.info("üìù –ù–µ–º–∞—î –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤ –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è")
            return 0
        
        logger.info(f"üíæ –ó–±–µ—Ä—ñ–≥–∞—î–º–æ {len(candidates)} –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤...")
        
        saved_count = 0
        
        try:
            with psycopg2.connect(self.db_connection_string) as conn:
                with conn.cursor() as cur:
                    for candidate in candidates:
                        try:
                            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –º–µ—Ç–æ–¥ –∑ BrandManager
                            from datetime import datetime
                            
                            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —ñ—Å–Ω—É—î –∫–∞–Ω–¥–∏–¥–∞—Ç
                            cur.execute("""
                                SELECT candidate_id FROM osm_ukraine.brand_candidates
                                WHERE name = %s
                            """, (candidate['name'],))
                            
                            if cur.fetchone():
                                # –û–Ω–æ–≤–ª—é—î–º–æ —ñ—Å–Ω—É—é—á–∏–π
                                cur.execute("""
                                    UPDATE osm_ukraine.brand_candidates 
                                    SET frequency = %s,
                                        last_seen = NOW(),
                                        locations = %s,
                                        categories = %s,
                                        confidence_score = %s,
                                        suggested_canonical_name = %s,
                                        suggested_functional_group = %s,
                                        suggested_influence_weight = %s,
                                        suggested_format = %s,
                                        recommendation_reason = %s
                                    WHERE name = %s
                                """, (
                                    candidate['frequency'],
                                    candidate['locations'],
                                    candidate['categories'],
                                    candidate['confidence_score'],
                                    candidate['suggested_canonical_name'],
                                    candidate['suggested_functional_group'],
                                    candidate['suggested_influence_weight'],
                                    candidate['suggested_format'],
                                    candidate['recommendation_reason'],
                                    candidate['name']
                                ))
                            else:
                                # –°—Ç–≤–æ—Ä—é—î–º–æ –Ω–æ–≤–∏–π
                                cur.execute("""
                                    INSERT INTO osm_ukraine.brand_candidates (
                                        name, frequency, first_seen, last_seen, locations, categories, 
                                        status, confidence_score, suggested_canonical_name,
                                        suggested_functional_group, suggested_influence_weight,
                                        suggested_format, recommendation_reason
                                    ) VALUES (
                                        %s, %s, NOW(), NOW(), %s, %s, 'new', %s, %s, %s, %s, %s, %s
                                    )
                                """, (
                                    candidate['name'],
                                    candidate['frequency'],
                                    candidate['locations'],
                                    candidate['categories'],
                                    candidate['confidence_score'],
                                    candidate['suggested_canonical_name'],
                                    candidate['suggested_functional_group'],
                                    candidate['suggested_influence_weight'],
                                    candidate['suggested_format'],
                                    candidate['recommendation_reason']
                                ))
                            
                            saved_count += 1
                            
                        except Exception as e:
                            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ '{candidate['name']}': {e}")
                            self.stats['errors'] += 1
                            continue
                    
                    conn.commit()
                    
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è: {e}")
            raise
        
        logger.info(f"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ {saved_count} –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤")
        return saved_count
    
    def _generate_summary_report(self, execution_time) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –ø—ñ–¥—Å—É–º–∫–æ–≤–æ–≥–æ –∑–≤—ñ—Ç—É"""
        logger.info("üìä –ì–µ–Ω–µ—Ä—É—î–º–æ –ø—ñ–¥—Å—É–º–∫–æ–≤–∏–π –∑–≤—ñ—Ç...")
        
        report = {
            'execution_time': str(execution_time),
            'statistics': self.stats.copy(),
            'top_candidates': self._get_top_candidates(10),
            'timestamp': datetime.now().isoformat()
        }
        
        # Console output
        print("\n" + "="*60)
        print("üîç BRAND CANDIDATE ANALYSIS RESULTS")
        print("="*60)
        print(f"üìä Regions analyzed: {self.stats['regions_processed']}")
        print(f"üìà Total POI processed: {self.stats['poi_analyzed']:,}")
        print(f"üè∑Ô∏è  Unique names found: {self.stats['unique_names_found']:,}")
        print(f"üö´ Generic names filtered: {self.stats['generic_filtered']:,}")
        print(f"‚úÖ Existing brand matches: {self.stats['existing_brand_matches']:,}")
        print(f"üéØ Quality candidates: {self.stats['quality_candidates']}")
        print(f"üè¢ Network candidates (2+ regions): {self.stats['network_candidates']}")
        print(f"üíæ Saved to database: {self.stats['saved_candidates']}")
        print(f"‚ùå Errors encountered: {self.stats['errors']}")
        print(f"‚è±Ô∏è  Execution time: {execution_time}")
        
        # Top candidates
        if report['top_candidates']:
            print(f"\nüèÜ TOP {len(report['top_candidates'])} CANDIDATES:")
            for i, candidate in enumerate(report['top_candidates'], 1):
                print(f"  {i:2d}. \"{candidate['name']}\" - {candidate['frequency']} locations, "
                      f"{len(candidate['locations'])} regions (conf: {candidate['confidence_score']:.3f})")
        
        print(f"\n‚úÖ Analysis completed successfully!")
        print("="*60)
        
        return report
    
    def _get_top_candidates(self, limit: int = 10) -> List[Dict[str, Any]]:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è —Ç–æ–ø –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤ –¥–ª—è –∑–≤—ñ—Ç—É"""
        top_candidates = []
        
        # –°–æ—Ä—Ç—É—î–º–æ –ø–æ –≤–ø–µ–≤–Ω–µ–Ω–æ—Å—Ç—ñ
        sorted_candidates = sorted(
            self.aggregated_candidates.items(),
            key=lambda x: x[1]['confidence_score'],
            reverse=True
        )
        
        for name, data in sorted_candidates[:limit]:
            if data['confidence_score'] >= CONFIG['analysis']['min_confidence']:
                top_candidates.append({
                    'name': name,
                    'frequency': data['total_frequency'],
                    'locations': data['regions'],
                    'confidence_score': data['confidence_score'],
                    'is_network': data['is_network_candidate']
                })
        
        return top_candidates


def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è"""
    try:
        # –°—Ç–≤–æ—Ä—é—î–º–æ –∞–Ω–∞–ª—ñ–∑–∞—Ç–æ—Ä
        finder = BrandCandidateFinder(DB_CONNECTION_STRING)
        
        # –ó–∞–ø—É—Å–∫–∞—î–º–æ –∞–Ω–∞–ª—ñ–∑
        report = finder.run_analysis()
        
        # –û–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ –º–æ–∂–µ–º–æ –∑–±–µ—Ä–µ–≥—Ç–∏ –∑–≤—ñ—Ç —É —Ñ–∞–π–ª
        # with open(f'brand_candidates_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json', 'w') as f:
        #     json.dump(report, f, indent=2, ensure_ascii=False)
        
        return 0
        
    except Exception as e:
        logger.error(f"üí• –§–∞—Ç–∞–ª—å–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)