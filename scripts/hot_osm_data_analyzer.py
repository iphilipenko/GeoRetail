#!/usr/bin/env python3
"""
Corrected HOT OSM Data Analyzer
–í–∏–ø—Ä–∞–≤–ª–µ–Ω–∏–π –∞–Ω–∞–ª—ñ–∑–∞—Ç–æ—Ä –¥–ª—è HOT OSM –µ–∫—Å–ø–æ—Ä—Ç—ñ–≤ –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–º —Ä–æ–∑—É–º—ñ–Ω–Ω—è–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏
"""

import geopandas as gpd
import pandas as pd
import json
import sqlite3
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import logging
from collections import defaultdict, Counter
import numpy as np
from datetime import datetime
import ast
import re

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class CorrectedHOTOSMAnalyzer:
    """–í–∏–ø—Ä–∞–≤–ª–µ–Ω–∏–π –∞–Ω–∞–ª—ñ–∑–∞—Ç–æ—Ä –¥–ª—è HOT OSM –µ–∫—Å–ø–æ—Ä—Ç—ñ–≤"""
    
    def __init__(self, data_directory: str):
        self.data_directory = Path(data_directory)
        self.analysis_results = {}
        
    def analyze_hot_osm_file(self, gpkg_path: Path, sample_size: int = 10000) -> Dict[str, Any]:
        """–ü–æ–≤–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ HOT OSM —Ñ–∞–π–ª—É –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–º —Ä–æ–∑—É–º—ñ–Ω–Ω—è–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏"""
        
        logger.info(f"üìä –ê–Ω–∞–ª—ñ–∑ HOT OSM —Ñ–∞–π–ª—É: {gpkg_path.name}")
        
        analysis = {
            'file_info': {
                'filename': gpkg_path.name,
                'size_mb': round(gpkg_path.stat().st_size / (1024 * 1024), 2),
                'region_name': self._extract_region_name(gpkg_path.name)
            },
            'data_structure': {},
            'osm_content_analysis': {},
            'spatial_analysis': {},
            'tag_analysis': {},
            'geometry_analysis': {},
            'postgis_schema_recommendations': {},
            'h3_integration_plan': {},
            'performance_estimates': {}
        }
        
        try:
            # 1. –û—Ç—Ä–∏–º–∞–Ω–Ω—è –æ—Å–Ω–æ–≤–Ω–æ—ó —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –ø—Ä–æ —Ç–∞–±–ª–∏—Ü—é
            table_name = self._get_main_table_name(gpkg_path)
            analysis['data_structure'] = self._analyze_table_structure(gpkg_path, table_name)
            
            # 2. –ê–Ω–∞–ª—ñ–∑ –≤–∏–±—ñ—Ä–∫–∏ –¥–∞–Ω–∏—Ö
            sample_data = self._load_data_sample(gpkg_path, table_name, sample_size)
            
            if not sample_data.empty:
                # 3. –ê–Ω–∞–ª—ñ–∑ OSM –∫–æ–Ω—Ç–µ–Ω—Ç—É
                analysis['osm_content_analysis'] = self._analyze_osm_content(sample_data)
                
                # 4. –ü—Ä–æ—Å—Ç–æ—Ä–æ–≤–∏–π –∞–Ω–∞–ª—ñ–∑
                analysis['spatial_analysis'] = self._analyze_spatial_data(sample_data)
                
                # 5. –ê–Ω–∞–ª—ñ–∑ —Ç–µ–≥—ñ–≤
                analysis['tag_analysis'] = self._analyze_osm_tags(sample_data)
                
                # 6. –ê–Ω–∞–ª—ñ–∑ –≥–µ–æ–º–µ—Ç—Ä—ñ—ó
                analysis['geometry_analysis'] = self._analyze_geometry_distribution(sample_data)
                
                # 7. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó PostGIS —Å—Ö–µ–º–∏
                analysis['postgis_schema_recommendations'] = self._create_postgis_schema_recommendations(
                    analysis, table_name
                )
                
                # 8. –ü–ª–∞–Ω H3 —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—ó
                analysis['h3_integration_plan'] = self._create_h3_integration_plan(analysis)
                
                # 9. –û—Ü—ñ–Ω–∫–∏ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
                analysis['performance_estimates'] = self._estimate_performance(analysis)
            
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∞–Ω–∞–ª—ñ–∑—É {gpkg_path.name}: {e}")
            analysis['error'] = str(e)
            
        return analysis
    
    def _extract_region_name(self, filename: str) -> str:
        """–í–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –Ω–∞–∑–≤–∏ —Ä–µ–≥—ñ–æ–Ω—É –∑ –Ω–∞–∑–≤–∏ —Ñ–∞–π–ª—É"""
        if filename.startswith('UA_MAP_'):
            region_name = filename[7:]  # –≤–∏–¥–∞–ª—è—î–º–æ 'UA_MAP_'
        else:
            region_name = filename
        
        if region_name.endswith('.gpkg'):
            region_name = region_name[:-5]  # –≤–∏–¥–∞–ª—è—î–º–æ '.gpkg'
        
        return region_name
    
    def _get_main_table_name(self, gpkg_path: Path) -> str:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è –Ω–∞–∑–≤–∏ –æ—Å–Ω–æ–≤–Ω–æ—ó —Ç–∞–±–ª–∏—Ü—ñ –∑ –¥–∞–Ω–∏–º–∏"""
        try:
            with sqlite3.connect(gpkg_path) as conn:
                cursor = conn.cursor()
                
                # –®—É–∫–∞—î–º–æ —Ç–∞–±–ª–∏—Ü—é –∑ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ—é –∫—ñ–ª—å–∫—ñ—Å—Ç—é –∑–∞–ø–∏—Å—ñ–≤ (–∫—Ä—ñ–º —Å–ª—É–∂–±–æ–≤–∏—Ö)
                cursor.execute("""
                    SELECT name FROM sqlite_master 
                    WHERE type='table' 
                    AND name NOT LIKE 'gpkg_%' 
                    AND name NOT LIKE 'rtree_%' 
                    AND name NOT LIKE 'sqlite_%'
                    ORDER BY name
                """)
                
                tables = cursor.fetchall()
                if tables:
                    # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –ø–µ—Ä—à—É –∑–Ω–∞–π–¥–µ–Ω—É —Ç–∞–±–ª–∏—Ü—é (–∑–∞–∑–≤–∏—á–∞–π —Ü–µ UA_MAP_*)
                    return tables[0][0]
                else:
                    raise ValueError("–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –æ—Å–Ω–æ–≤–Ω—É —Ç–∞–±–ª–∏—Ü—é –∑ –¥–∞–Ω–∏–º–∏")
                    
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –Ω–∞–∑–≤–∏ —Ç–∞–±–ª–∏—Ü—ñ: {e}")
            return "unknown_table"
    
    def _analyze_table_structure(self, gpkg_path: Path, table_name: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª—ñ–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ –æ—Å–Ω–æ–≤–Ω–æ—ó —Ç–∞–±–ª–∏—Ü—ñ"""
        
        structure = {
            'table_name': table_name,
            'total_records': 0,
            'columns': {},
            'indexes': [],
            'spatial_info': {}
        }
        
        try:
            with sqlite3.connect(gpkg_path) as conn:
                cursor = conn.cursor()
                
                # –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–∞–ø–∏—Å—ñ–≤
                cursor.execute(f"SELECT COUNT(*) FROM \"{table_name}\"")
                structure['total_records'] = cursor.fetchone()[0]
                
                # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–æ–ª–æ–Ω–æ–∫
                cursor.execute(f"PRAGMA table_info(\"{table_name}\")")
                columns_info = cursor.fetchall()
                
                for col_info in columns_info:
                    col_name = col_info[1]
                    structure['columns'][col_name] = {
                        'type': col_info[2],
                        'nullable': not col_info[3],
                        'default': col_info[4],
                        'primary_key': bool(col_info[5])
                    }
                
                # –ü—Ä–æ—Å—Ç–æ—Ä–æ–≤–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –∑ gpkg_geometry_columns
                cursor.execute("""
                    SELECT geometry_type_name, srs_id 
                    FROM gpkg_geometry_columns 
                    WHERE table_name = ?
                """, (table_name,))
                
                spatial_result = cursor.fetchone()
                if spatial_result:
                    structure['spatial_info'] = {
                        'geometry_type': spatial_result[0],
                        'srs_id': spatial_result[1],
                        'has_spatial_index': True  # R-tree –∑–∞–≤–∂–¥–∏ —î –≤ GPKG
                    }
                
                # Bounds –∑ gpkg_contents
                cursor.execute("""
                    SELECT min_x, min_y, max_x, max_y 
                    FROM gpkg_contents 
                    WHERE table_name = ?
                """, (table_name,))
                
                bounds_result = cursor.fetchone()
                if bounds_result and all(b is not None for b in bounds_result):
                    structure['spatial_info']['bounds'] = {
                        'minx': bounds_result[0],
                        'miny': bounds_result[1],
                        'maxx': bounds_result[2],
                        'maxy': bounds_result[3]
                    }
                
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –∞–Ω–∞–ª—ñ–∑—É —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ —Ç–∞–±–ª–∏—Ü—ñ: {e}")
            structure['error'] = str(e)
            
        return structure
    
    def _load_data_sample(self, gpkg_path: Path, table_name: str, sample_size: int) -> gpd.GeoDataFrame:
        """–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –≤–∏–±—ñ—Ä–∫–∏ –¥–∞–Ω–∏—Ö –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É"""
        
        try:
            # –ß–∏—Ç–∞—î–º–æ –≤–∏–±—ñ—Ä–∫—É —á–µ—Ä–µ–∑ GeoPandas
            logger.info(f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –≤–∏–±—ñ—Ä–∫–∏ {sample_size} –∑–∞–ø–∏—Å—ñ–≤...")
            
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ SQL –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –≤–∏–±—ñ—Ä–∫–∏
            sql_query = f"""
                SELECT * FROM "{table_name}" 
                WHERE geom IS NOT NULL 
                LIMIT {sample_size}
            """
            
            gdf = gpd.read_file(gpkg_path, sql=sql_query)
            logger.info(f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(gdf)} –∑–∞–ø–∏—Å—ñ–≤ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É")
            
            return gdf
            
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –≤–∏–±—ñ—Ä–∫–∏: {e}")
            return gpd.GeoDataFrame()
    
    def _analyze_osm_content(self, gdf: gpd.GeoDataFrame) -> Dict[str, Any]:
        """–ê–Ω–∞–ª—ñ–∑ OSM –∫–æ–Ω—Ç–µ–Ω—Ç—É"""
        
        content_analysis = {
            'osm_id_analysis': {},
            'osm_type_distribution': {},
            'version_analysis': {},
            'temporal_analysis': {},
            'user_analysis': {}
        }
        
        try:
            # –ê–Ω–∞–ª—ñ–∑ OSM ID
            if 'osm_id' in gdf.columns:
                osm_ids = gdf['osm_id'].dropna()
                content_analysis['osm_id_analysis'] = {
                    'total_unique_ids': int(osm_ids.nunique()),
                    'id_range': {
                        'min': int(osm_ids.min()),
                        'max': int(osm_ids.max())
                    },
                    'negative_ids_count': int((osm_ids < 0).sum()),  # –ó–∞–∑–≤–∏—á–∞–π relation members
                    'positive_ids_count': int((osm_ids > 0).sum())
                }
            
            # –ê–Ω–∞–ª—ñ–∑ —Ç–∏–ø—ñ–≤ OSM –æ–±'—î–∫—Ç—ñ–≤
            if 'osm_type' in gdf.columns:
                osm_types = gdf['osm_type'].value_counts()
                content_analysis['osm_type_distribution'] = {
                    'types': osm_types.to_dict(),
                    'dominant_type': osm_types.index[0] if len(osm_types) > 0 else None
                }
            
            # –ê–Ω–∞–ª—ñ–∑ –≤–µ—Ä—Å—ñ–π
            if 'version' in gdf.columns:
                versions = gdf['version'].dropna()
                content_analysis['version_analysis'] = {
                    'version_range': {
                        'min': int(versions.min()) if len(versions) > 0 else None,
                        'max': int(versions.max()) if len(versions) > 0 else None
                    },
                    'avg_version': float(versions.mean()) if len(versions) > 0 else None,
                    'single_version_objects': int((versions == 1).sum())
                }
            
            # –¢–µ–º–ø–æ—Ä–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑
            if 'timestamp' in gdf.columns:
                timestamps = pd.to_datetime(gdf['timestamp'], errors='coerce').dropna()
                if len(timestamps) > 0:
                    content_analysis['temporal_analysis'] = {
                        'date_range': {
                            'earliest': timestamps.min().isoformat(),
                            'latest': timestamps.max().isoformat()
                        },
                        'data_freshness_days': (datetime.now() - timestamps.max()).days,
                        'temporal_span_days': (timestamps.max() - timestamps.min()).days
                    }
            
            # –ê–Ω–∞–ª—ñ–∑ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤
            if 'user' in gdf.columns:
                users = gdf['user'].dropna()
                user_counts = users.value_counts()
                content_analysis['user_analysis'] = {
                    'unique_contributors': int(users.nunique()),
                    'top_contributors': user_counts.head(10).to_dict(),
                    'single_edit_users': int((user_counts == 1).sum())
                }
            
        except Exception as e:
            content_analysis['error'] = str(e)
            
        return content_analysis
    
    def _analyze_spatial_data(self, gdf: gpd.GeoDataFrame) -> Dict[str, Any]:
        """–ü—Ä–æ—Å—Ç–æ—Ä–æ–≤–∏–π –∞–Ω–∞–ª—ñ–∑ –¥–∞–Ω–∏—Ö"""
        
        spatial_analysis = {
            'coordinate_system': {},
            'spatial_extent': {},
            'density_analysis': {},
            'geometric_validity': {}
        }
        
        try:
            if 'geometry' not in gdf.columns or gdf.geometry.empty:
                return spatial_analysis
            
            # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç–Ω–∞ —Å–∏—Å—Ç–µ–º–∞
            spatial_analysis['coordinate_system'] = {
                'crs': str(gdf.crs),
                'epsg_code': gdf.crs.to_epsg() if gdf.crs else None,
                'is_geographic': gdf.crs.is_geographic if gdf.crs else None
            }
            
            # –ü—Ä–æ—Å—Ç–æ—Ä–æ–≤–µ –ø–æ–∫—Ä–∏—Ç—Ç—è
            bounds = gdf.total_bounds
            spatial_analysis['spatial_extent'] = {
                'bounds': {
                    'minx': float(bounds[0]),
                    'miny': float(bounds[1]),
                    'maxx': float(bounds[2]),
                    'maxy': float(bounds[3])
                },
                'center': {
                    'lat': float((bounds[1] + bounds[3]) / 2),
                    'lon': float((bounds[0] + bounds[2]) / 2)
                },
                'extent_degrees': {
                    'width': float(bounds[2] - bounds[0]),
                    'height': float(bounds[3] - bounds[1])
                }
            }
            
            # –ü—Ä–∏–±–ª–∏–∑–Ω–∞ –ø–ª–æ—â–∞ –ø–æ–∫—Ä–∏—Ç—Ç—è (–¥–ª—è –º–∞–ª–∏—Ö –æ–±–ª–∞—Å—Ç–µ–π –º–æ–∂–Ω–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ –ø—Ä–æ—Å—Ç—É —Ñ–æ—Ä–º—É–ª—É)
            area_deg2 = (bounds[2] - bounds[0]) * (bounds[3] - bounds[1])
            # –ü—Ä–∏–±–ª–∏–∑–Ω–µ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—è –≤ –∫–º¬≤ –¥–ª—è —à–∏—Ä–æ—Ç –±–ª–∏–∑—å–∫–æ 50¬∞ (–£–∫—Ä–∞—ó–Ω–∞)
            lat_center = (bounds[1] + bounds[3]) / 2
            km_per_deg_lat = 111.0
            km_per_deg_lon = 111.0 * np.cos(np.radians(lat_center))
            area_km2 = area_deg2 * km_per_deg_lat * km_per_deg_lon
            
            spatial_analysis['spatial_extent']['approximate_area_km2'] = float(area_km2)
            
            # –ê–Ω–∞–ª—ñ–∑ —â—ñ–ª—å–Ω–æ—Å—Ç—ñ
            feature_density = len(gdf) / area_km2 if area_km2 > 0 else 0
            spatial_analysis['density_analysis'] = {
                'features_per_km2': float(feature_density),
                'total_features_in_sample': len(gdf)
            }
            
            # –í–∞–ª—ñ–¥–Ω—ñ—Å—Ç—å –≥–µ–æ–º–µ—Ç—Ä—ñ—ó
            valid_geoms = gdf.geometry.is_valid
            spatial_analysis['geometric_validity'] = {
                'valid_geometries': int(valid_geoms.sum()),
                'invalid_geometries': int((~valid_geoms).sum()),
                'validity_ratio': float(valid_geoms.mean()),
                'null_geometries': int(gdf.geometry.isna().sum())
            }
            
        except Exception as e:
            spatial_analysis['error'] = str(e)
            
        return spatial_analysis
    
    def _analyze_osm_tags(self, gdf: gpd.GeoDataFrame) -> Dict[str, Any]:
        """–î–µ—Ç–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ OSM —Ç–µ–≥—ñ–≤ –∑ –ø–æ–ª—è 'tags'"""
        
        tag_analysis = {
            'tags_structure': {},
            'key_retail_tags': {},
            'all_tag_keys': {},
            'tag_patterns': {},
            'retail_relevance': {}
        }
        
        try:
            if 'tags' not in gdf.columns:
                tag_analysis['error'] = "–ö–æ–ª–æ–Ω–∫–∞ 'tags' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞"
                return tag_analysis
            
            # –ü–∞—Ä—Å–∏–Ω–≥ —Ç–µ–≥—ñ–≤
            parsed_tags = []
            valid_tags_count = 0
            
            for tags_str in gdf['tags'].dropna():
                try:
                    if tags_str and tags_str.strip():
                        # HOT –µ–∫—Å–ø–æ—Ä—Ç–∏ –∑–∞–∑–≤–∏—á–∞–π –∑–±–µ—Ä—ñ–≥–∞—é—Ç—å —Ç–µ–≥–∏ —è–∫ JSON –∞–±–æ key=value
                        if tags_str.startswith('{'):
                            # JSON —Ñ–æ—Ä–º–∞—Ç
                            tags_dict = json.loads(tags_str)
                        else:
                            # –Ü–Ω—à—ñ —Ñ–æ—Ä–º–∞—Ç–∏ - —Å–ø—Ä–æ–±—É—î–º–æ —Ä—ñ–∑–Ω—ñ –≤–∞—Ä—ñ–∞–Ω—Ç–∏
                            tags_dict = self._parse_tags_string(tags_str)
                        
                        if tags_dict:
                            parsed_tags.append(tags_dict)
                            valid_tags_count += 1
                except Exception:
                    continue
            
            tag_analysis['tags_structure'] = {
                'total_records_with_tags': valid_tags_count,
                'parsing_success_rate': float(valid_tags_count / len(gdf)) if len(gdf) > 0 else 0
            }
            
            if not parsed_tags:
                tag_analysis['error'] = "–ù–µ –≤–¥–∞–ª–æ—Å—è —Ä–æ–∑–ø–∞—Ä—Å–∏—Ç–∏ –∂–æ–¥–Ω–æ–≥–æ —Ç–µ–≥—É"
                return tag_analysis
            
            # –ó–±—ñ—Ä –≤—Å—ñ—Ö –∫–ª—é—á—ñ–≤ —Ç–µ–≥—ñ–≤
            all_keys = Counter()
            key_value_pairs = defaultdict(Counter)
            
            for tags_dict in parsed_tags:
                for key, value in tags_dict.items():
                    all_keys[key] += 1
                    if value and str(value).strip():
                        key_value_pairs[key][str(value)] += 1
            
            # –¢–æ–ø –∫–ª—é—á—ñ —Ç–µ–≥—ñ–≤
            tag_analysis['all_tag_keys'] = {
                'total_unique_keys': len(all_keys),
                'top_keys': dict(all_keys.most_common(30)),
                'keys_with_single_occurrence': sum(1 for count in all_keys.values() if count == 1)
            }
            
            # –ö–ª—é—á–æ–≤—ñ —Ç–µ–≥–∏ –¥–ª—è —Ä–µ—Ç–µ–π–ª—É
            retail_tags = [
                'amenity', 'shop', 'building', 'landuse', 'highway', 'railway',
                'natural', 'leisure', 'tourism', 'office', 'name', 'brand',
                'addr:housenumber', 'addr:street', 'addr:city', 'addr:postcode',
                'opening_hours', 'phone', 'website', 'cuisine', 'level'
            ]
            
            for tag_key in retail_tags:
                if tag_key in all_keys:
                    values = key_value_pairs[tag_key]
                    tag_analysis['key_retail_tags'][tag_key] = {
                        'total_occurrences': all_keys[tag_key],
                        'occurrence_rate': float(all_keys[tag_key] / valid_tags_count),
                        'unique_values': len(values),
                        'top_values': dict(values.most_common(10))
                    }
            
            # –ü–∞—Ç—Ç–µ—Ä–Ω–∏ —Ç–µ–≥—ñ–≤
            tag_analysis['tag_patterns'] = self._analyze_tag_patterns(all_keys, key_value_pairs)
            
            # –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—å –¥–ª—è —Ä–µ—Ç–µ–π–ª—É
            retail_score = self._calculate_retail_relevance(tag_analysis['key_retail_tags'])
            tag_analysis['retail_relevance'] = retail_score
            
        except Exception as e:
            tag_analysis['error'] = str(e)
            
        return tag_analysis
    
    def _parse_tags_string(self, tags_str: str) -> Dict[str, str]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ä—è–¥–∫–∞ —Ç–µ–≥—ñ–≤ —É —Å–ª–æ–≤–Ω–∏–∫"""
        
        tags_dict = {}
        
        try:
            # –°–ø—Ä–æ–±–∞ JSON
            if tags_str.startswith('{'):
                return json.loads(tags_str)
            
            # –°–ø—Ä–æ–±–∞ eval –¥–ª—è Python dict literal
            if '=>' in tags_str:
                # Perl/Ruby —Å—Ç–∏–ª—å —Ö–µ—à—ñ–≤
                tags_str = tags_str.replace('=>', ':')
                tags_str = re.sub(r'(\w+):', r'"\1":', tags_str)
                return json.loads(tags_str)
            
            # Key=value pairs —Ä–æ–∑–¥—ñ–ª–µ–Ω—ñ –∫–æ–º–∞–º–∏ –∞–±–æ –Ω–æ–≤–∏–º–∏ —Ä—è–¥–∫–∞–º–∏
            if '=' in tags_str:
                pairs = re.split(r'[,\n\r]+', tags_str)
                for pair in pairs:
                    if '=' in pair:
                        key, value = pair.split('=', 1)
                        tags_dict[key.strip().strip('"\'')] = value.strip().strip('"\'')
            
        except Exception:
            pass
            
        return tags_dict
    
    def _analyze_tag_patterns(self, all_keys: Counter, key_value_pairs: Dict) -> Dict[str, Any]:
        """–ê–Ω–∞–ª—ñ–∑ –ø–∞—Ç—Ç–µ—Ä–Ω—ñ–≤ —É —Ç–µ–≥–∞—Ö"""
        
        patterns = {
            'address_completeness': 0,
            'multilingual_names': 0,
            'commercial_indicators': 0,
            'accessibility_info': 0
        }
        
        try:
            # –ê–¥—Ä–µ—Å–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è
            address_keys = ['addr:housenumber', 'addr:street', 'addr:city', 'addr:postcode']
            address_present = sum(1 for key in address_keys if key in all_keys)
            patterns['address_completeness'] = address_present / len(address_keys)
            
            # –ë–∞–≥–∞—Ç–æ–º–æ–≤–Ω—ñ –Ω–∞–∑–≤–∏
            name_keys = [key for key in all_keys if key.startswith('name:')]
            patterns['multilingual_names'] = len(name_keys)
            
            # –ö–æ–º–µ—Ä—Ü—ñ–π–Ω—ñ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏
            commercial_keys = ['shop', 'amenity', 'office', 'commercial', 'brand', 'operator']
            commercial_present = sum(1 for key in commercial_keys if key in all_keys)
            patterns['commercial_indicators'] = commercial_present
            
            # –Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –¥–æ—Å—Ç—É–ø–Ω—ñ—Å—Ç—å
            accessibility_keys = [key for key in all_keys if 'wheelchair' in key or 'access' in key]
            patterns['accessibility_info'] = len(accessibility_keys)
            
        except Exception:
            pass
            
        return patterns
    
    def _calculate_retail_relevance(self, key_retail_tags: Dict) -> Dict[str, Any]:
        """–†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—ñ –¥–ª—è —Ä–µ—Ç–µ–π–ª –∞–Ω–∞–ª—ñ–∑—É"""
        
        relevance = {
            'overall_score': 0.0,
            'poi_richness': 0.0,
            'address_quality': 0.0,
            'commercial_density': 0.0,
            'suitability_assessment': 'unknown'
        }
        
        try:
            scores = []
            
            # POI –±–∞–≥–∞—Ç—Å—Ç–≤–æ
            poi_tags = ['amenity', 'shop', 'office', 'leisure', 'tourism']
            poi_score = sum(key_retail_tags.get(tag, {}).get('occurrence_rate', 0) for tag in poi_tags)
            relevance['poi_richness'] = min(poi_score, 1.0)
            scores.append(relevance['poi_richness'])
            
            # –Ø–∫—ñ—Å—Ç—å –∞–¥—Ä–µ—Å
            address_tags = ['addr:housenumber', 'addr:street', 'addr:city']
            address_score = sum(key_retail_tags.get(tag, {}).get('occurrence_rate', 0) for tag in address_tags) / len(address_tags)
            relevance['address_quality'] = min(address_score, 1.0)
            scores.append(relevance['address_quality'])
            
            # –ö–æ–º–µ—Ä—Ü—ñ–π–Ω–∞ —â—ñ–ª—å–Ω—ñ—Å—Ç—å
            commercial_tags = ['shop', 'amenity', 'building']
            commercial_score = sum(key_retail_tags.get(tag, {}).get('occurrence_rate', 0) for tag in commercial_tags) / len(commercial_tags)
            relevance['commercial_density'] = min(commercial_score, 1.0)
            scores.append(relevance['commercial_density'])
            
            # –ó–∞–≥–∞–ª—å–Ω–∞ –æ—Ü—ñ–Ω–∫–∞
            relevance['overall_score'] = np.mean(scores) if scores else 0.0
            
            # –û—Ü—ñ–Ω–∫–∞ –ø—Ä–∏–¥–∞—Ç–Ω–æ—Å—Ç—ñ
            if relevance['overall_score'] > 0.7:
                relevance['suitability_assessment'] = 'excellent'
            elif relevance['overall_score'] > 0.5:
                relevance['suitability_assessment'] = 'good'
            elif relevance['overall_score'] > 0.3:
                relevance['suitability_assessment'] = 'moderate'
            else:
                relevance['suitability_assessment'] = 'poor'
            
        except Exception:
            relevance['suitability_assessment'] = 'error'
            
        return relevance
    
    def _analyze_geometry_distribution(self, gdf: gpd.GeoDataFrame) -> Dict[str, Any]:
        """–ê–Ω–∞–ª—ñ–∑ —Ä–æ–∑–ø–æ–¥—ñ–ª—É –≥–µ–æ–º–µ—Ç—Ä—ñ–π"""
        
        geometry_analysis = {
            'geometry_types': {},
            'complexity_analysis': {},
            'spatial_clustering': {},
            'h3_compatibility': {}
        }
        
        try:
            if 'geometry' not in gdf.columns or gdf.geometry.empty:
                return geometry_analysis
            
            # –¢–∏–ø–∏ –≥–µ–æ–º–µ—Ç—Ä—ñ–π
            geom_types = gdf.geometry.geom_type.value_counts()
            geometry_analysis['geometry_types'] = {
                'distribution': geom_types.to_dict(),
                'primary_type': geom_types.index[0] if len(geom_types) > 0 else None,
                'type_diversity': len(geom_types)
            }
            
            # –ê–Ω–∞–ª—ñ–∑ —Å–∫–ª–∞–¥–Ω–æ—Å—Ç—ñ
            complexity_stats = []
            for geom in gdf.geometry.dropna().head(1000):  # –û–±–º–µ–∂—É—î–º–æ –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ
                try:
                    if hasattr(geom, 'coords'):
                        complexity_stats.append(len(list(geom.coords)))
                    elif hasattr(geom, 'exterior'):
                        complexity_stats.append(len(list(geom.exterior.coords)))
                    elif hasattr(geom, 'geoms'):
                        total_coords = sum(len(list(g.coords)) if hasattr(g, 'coords') 
                                         else len(list(g.exterior.coords)) if hasattr(g, 'exterior') 
                                         else 0 for g in geom.geoms)
                        complexity_stats.append(total_coords)
                except:
                    continue
            
            if complexity_stats:
                geometry_analysis['complexity_analysis'] = {
                    'avg_vertices': float(np.mean(complexity_stats)),
                    'max_vertices': int(np.max(complexity_stats)),
                    'min_vertices': int(np.min(complexity_stats)),
                    'complexity_variance': float(np.var(complexity_stats))
                }
            
            # H3 —Å—É–º—ñ—Å–Ω—ñ—Å—Ç—å
            primary_type = geometry_analysis['geometry_types']['primary_type']
            if primary_type:
                geometry_analysis['h3_compatibility'] = self._assess_h3_compatibility(primary_type, len(gdf))
            
        except Exception as e:
            geometry_analysis['error'] = str(e)
            
        return geometry_analysis
    
    def _assess_h3_compatibility(self, primary_geom_type: str, feature_count: int) -> Dict[str, Any]:
        """–û—Ü—ñ–Ω–∫–∞ —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ –∑ H3 —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—î—é"""
        
        compatibility = {
            'suitable': False,
            'recommended_resolutions': [],
            'indexing_strategy': None,
            'performance_expectation': 'unknown'
        }
        
        try:
            if primary_geom_type == 'Point':
                compatibility.update({
                    'suitable': True,
                    'recommended_resolutions': [8, 9, 10],
                    'indexing_strategy': 'direct_point_to_cell',
                    'performance_expectation': 'excellent'
                })
            elif primary_geom_type in ['Polygon', 'MultiPolygon']:
                compatibility.update({
                    'suitable': True,
                    'recommended_resolutions': [7, 8, 9],
                    'indexing_strategy': 'polygon_coverage',
                    'performance_expectation': 'good'
                })
            elif primary_geom_type in ['LineString', 'MultiLineString']:
                compatibility.update({
                    'suitable': True,
                    'recommended_resolutions': [8, 9],
                    'indexing_strategy': 'line_intersection',
                    'performance_expectation': 'moderate'
                })
            
            # –ö–æ—Ä–µ–∫—Ü—ñ—è –∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –æ–±'—î–∫—Ç—ñ–≤
            if feature_count > 1000000 and compatibility['performance_expectation'] != 'unknown':
                if compatibility['performance_expectation'] == 'excellent':
                    compatibility['performance_expectation'] = 'good'
                elif compatibility['performance_expectation'] == 'good':
                    compatibility['performance_expectation'] = 'moderate'
            
        except Exception:
            pass
            
        return compatibility
    
    def _create_postgis_schema_recommendations(self, analysis: Dict, table_name: str) -> Dict[str, Any]:
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π –¥–ª—è PostGIS —Å—Ö–µ–º–∏"""
        
        schema_recommendations = {
            'main_table': {},
            'tag_extraction_tables': {},
            'indexes': [],
            'materialized_views': [],
            'h3_integration': {},
            'partitioning_strategy': {}
        }
        
        try:
            data_structure = analysis.get('data_structure', {})
            tag_analysis = analysis.get('tag_analysis', {})
            geometry_analysis = analysis.get('geometry_analysis', {})
            
            total_records = data_structure.get('total_records', 0)
            region_name = analysis.get('file_info', {}).get('region_name', 'unknown')
            
            # –û—Å–Ω–æ–≤–Ω–∞ —Ç–∞–±–ª–∏—Ü—è
            schema_recommendations['main_table'] = {
                'table_name': f'osm_raw_{region_name.lower()}',
                'columns': [
                    'id SERIAL PRIMARY KEY',
                    'fid INTEGER UNIQUE',  # –û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π fid –∑ GPKG
                    'osm_id BIGINT',
                    'osm_type VARCHAR(20)',
                    'version INTEGER',
                    'changeset INTEGER',
                    'uid INTEGER',
                    'username VARCHAR(255)',
                    'timestamp TIMESTAMP WITH TIME ZONE',
                    'geom GEOMETRY(GEOMETRY, 4326)',  # –ó–º—ñ—à–∞–Ω—ñ —Ç–∏–ø–∏ –≥–µ–æ–º–µ—Ç—Ä—ñ—ó
                    'tags JSONB',  # –î–ª—è –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è –≤—Å—ñ—Ö —Ç–µ–≥—ñ–≤
                    'region_name VARCHAR(50) DEFAULT \'{}\' '.format(region_name),
                    'created_at TIMESTAMP DEFAULT NOW()',
                    'updated_at TIMESTAMP DEFAULT NOW()'
                ],
                'estimated_size_gb': round(total_records * 0.5 / 1000000, 2)  # –ü—Ä–∏–±–ª–∏–∑–Ω–∞ –æ—Ü—ñ–Ω–∫–∞
            }
            
            # H3 –∫–æ–ª–æ–Ω–∫–∏
            h3_columns = []
            for res in [7, 8, 9, 10]:
                h3_columns.append(f'h3_res_{res} VARCHAR(15)')
            
            schema_recommendations['main_table']['columns'].extend(h3_columns)
            
            # –¢–∞–±–ª–∏—Ü—ñ –¥–ª—è –≤–∏—Ç—è–≥–Ω–µ–Ω–Ω—è —Ç–µ–≥—ñ–≤
            retail_tags = tag_analysis.get('key_retail_tags', {})
            if retail_tags:
                # –°—Ç–≤–æ—Ä—é—î–º–æ normalized —Ç–∞–±–ª–∏—Ü—é –¥–ª—è —à–≤–∏–¥–∫–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤
                schema_recommendations['tag_extraction_tables']['osm_poi_normalized'] = {
                    'table_name': f'osm_poi_{region_name.lower()}',
                    'columns': [
                        'id SERIAL PRIMARY KEY',
                        'osm_raw_id INTEGER REFERENCES osm_raw_{} (id)'.format(region_name.lower()),
                        'osm_id BIGINT',
                        'geom GEOMETRY(GEOMETRY, 4326)',
                        'poi_type VARCHAR(50)',  # amenity, shop, etc.
                        'poi_value VARCHAR(255)', # restaurant, supermarket, etc.
                        'name VARCHAR(255)',
                        'brand VARCHAR(255)',
                        'addr_housenumber VARCHAR(20)',
                        'addr_street VARCHAR(255)',
                        'addr_city VARCHAR(255)',
                        'addr_postcode VARCHAR(20)',
                        'opening_hours TEXT',
                        'phone VARCHAR(50)',
                        'website VARCHAR(500)',
                        'cuisine VARCHAR(255)',
                        'level VARCHAR(20)',
                        'building VARCHAR(100)',
                        'landuse VARCHAR(100)',
                        'h3_res_8 VARCHAR(15)',
                        'h3_res_9 VARCHAR(15)',
                        'h3_res_10 VARCHAR(15)',
                        'created_at TIMESTAMP DEFAULT NOW()'
                    ],
                    'purpose': 'Normalized POI data for fast retail analysis'
                }
            
            # –Ü–Ω–¥–µ–∫—Å–∏
            main_table_name = schema_recommendations['main_table']['table_name']
            
            # –û—Å–Ω–æ–≤–Ω—ñ —ñ–Ω–¥–µ–∫—Å–∏
            schema_recommendations['indexes'].extend([
                f'CREATE INDEX idx_{main_table_name}_geom ON {main_table_name} USING GIST (geom)',
                f'CREATE INDEX idx_{main_table_name}_osm_id ON {main_table_name} (osm_id)',
                f'CREATE INDEX idx_{main_table_name}_osm_type ON {main_table_name} (osm_type)',
                f'CREATE INDEX idx_{main_table_name}_tags_gin ON {main_table_name} USING GIN (tags)',
                f'CREATE INDEX idx_{main_table_name}_region ON {main_table_name} (region_name)'
            ])
            
            # H3 —ñ–Ω–¥–µ–∫—Å–∏
            for res in [7, 8, 9, 10]:
                schema_recommendations['indexes'].append(
                    f'CREATE INDEX idx_{main_table_name}_h3_res_{res} ON {main_table_name} (h3_res_{res})'
                )
            
            # JSONB —ñ–Ω–¥–µ–∫—Å–∏ –¥–ª—è –∫–ª—é—á–æ–≤–∏—Ö —Ç–µ–≥—ñ–≤
            for tag_key in ['amenity', 'shop', 'building', 'landuse', 'highway', 'name']:
                if tag_key in retail_tags:
                    schema_recommendations['indexes'].append(
                        f"CREATE INDEX idx_{main_table_name}_tags_{tag_key} ON {main_table_name} USING GIN ((tags->'{tag_key}'))"
                    )
            
            # –ü–∞—Ä—Ç–∏—Ü—ñ–æ–Ω—É–≤–∞–Ω–Ω—è –¥–ª—è –≤–µ–ª–∏–∫–∏—Ö —Ç–∞–±–ª–∏—Ü—å
            if total_records > 5000000:  # 5–ú+ –∑–∞–ø–∏—Å—ñ–≤
                schema_recommendations['partitioning_strategy'] = {
                    'recommended': True,
                    'strategy': 'hash_partitioning_by_h3',
                    'partition_count': min(16, max(4, total_records // 1000000)),
                    'partition_key': 'h3_res_8',
                    'benefits': [
                        'Faster queries on H3 cells',
                        'Parallel processing',
                        'Better index performance'
                    ]
                }
            
            # –ú–∞—Ç–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è
            if total_records > 100000:
                schema_recommendations['materialized_views'].extend([
                    {
                        'name': f'mv_poi_summary_{region_name.lower()}',
                        'sql': f"""
                        CREATE MATERIALIZED VIEW mv_poi_summary_{region_name.lower()} AS
                        SELECT 
                            h3_res_8,
                            COUNT(*) as total_features,
                            COUNT(*) FILTER (WHERE tags->>'amenity' IS NOT NULL) as amenity_count,
                            COUNT(*) FILTER (WHERE tags->>'shop' IS NOT NULL) as shop_count,
                            COUNT(*) FILTER (WHERE tags->>'building' IS NOT NULL) as building_count,
                            ST_Centroid(ST_Collect(geom)) as center_point
                        FROM {main_table_name}
                        WHERE h3_res_8 IS NOT NULL
                        GROUP BY h3_res_8
                        """,
                        'refresh_schedule': 'daily',
                        'purpose': 'Fast H3-based aggregations for dashboards'
                    },
                    {
                        'name': f'mv_retail_density_{region_name.lower()}',
                        'sql': f"""
                        CREATE MATERIALIZED VIEW mv_retail_density_{region_name.lower()} AS
                        SELECT 
                            h3_res_9,
                            COUNT(*) FILTER (WHERE tags->>'shop' IN ('supermarket', 'convenience', 'mall')) as retail_count,
                            COUNT(*) FILTER (WHERE tags->>'amenity' IN ('restaurant', 'cafe', 'fast_food')) as food_count,
                            COUNT(*) FILTER (WHERE tags->>'building' = 'commercial') as commercial_buildings,
                            ST_Centroid(ST_Collect(geom)) as center_point
                        FROM {main_table_name}
                        WHERE h3_res_9 IS NOT NULL
                        GROUP BY h3_res_9
                        """,
                        'refresh_schedule': 'weekly',
                        'purpose': 'Retail density analysis for location intelligence'
                    }
                ])
            
        except Exception as e:
            schema_recommendations['error'] = str(e)
            
        return schema_recommendations
    
    def _create_h3_integration_plan(self, analysis: Dict) -> Dict[str, Any]:
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–ª–∞–Ω—É H3 —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—ó"""
        
        h3_plan = {
            'processing_strategy': {},
            'resolution_usage': {},
            'performance_optimization': {},
            'sql_functions_needed': []
        }
        
        try:
            geometry_analysis = analysis.get('geometry_analysis', {})
            data_structure = analysis.get('data_structure', {})
            
            total_records = data_structure.get('total_records', 0)
            primary_geom_type = geometry_analysis.get('geometry_types', {}).get('primary_type')
            
            # –°—Ç—Ä–∞—Ç–µ–≥—ñ—è –æ–±—Ä–æ–±–∫–∏
            if primary_geom_type == 'Point':
                h3_plan['processing_strategy'] = {
                    'method': 'direct_geocoding',
                    'sql_template': '''
                    UPDATE osm_raw_table SET 
                        h3_res_7 = h3_geo_to_h3(ST_Y(geom), ST_X(geom), 7),
                        h3_res_8 = h3_geo_to_h3(ST_Y(geom), ST_X(geom), 8),
                        h3_res_9 = h3_geo_to_h3(ST_Y(geom), ST_X(geom), 9),
                        h3_res_10 = h3_geo_to_h3(ST_Y(geom), ST_X(geom), 10)
                    WHERE geom IS NOT NULL;
                    ''',
                    'estimated_processing_time_hours': max(0.1, total_records / 1000000)
                }
            elif primary_geom_type in ['Polygon', 'MultiPolygon']:
                h3_plan['processing_strategy'] = {
                    'method': 'centroid_based',
                    'sql_template': '''
                    UPDATE osm_raw_table SET 
                        h3_res_7 = h3_geo_to_h3(ST_Y(ST_Centroid(geom)), ST_X(ST_Centroid(geom)), 7),
                        h3_res_8 = h3_geo_to_h3(ST_Y(ST_Centroid(geom)), ST_X(ST_Centroid(geom)), 8),
                        h3_res_9 = h3_geo_to_h3(ST_Y(ST_Centroid(geom)), ST_X(ST_Centroid(geom)), 9),
                        h3_res_10 = h3_geo_to_h3(ST_Y(ST_Centroid(geom)), ST_X(ST_Centroid(geom)), 10)
                    WHERE geom IS NOT NULL;
                    ''',
                    'estimated_processing_time_hours': max(0.2, total_records / 500000),
                    'alternative_method': 'polygon_coverage_for_large_areas'
                }
            else:
                h3_plan['processing_strategy'] = {
                    'method': 'mixed_geometry_handling',
                    'sql_template': '''
                    UPDATE osm_raw_table SET 
                        h3_res_7 = CASE 
                            WHEN ST_GeometryType(geom) = 'ST_Point' THEN h3_geo_to_h3(ST_Y(geom), ST_X(geom), 7)
                            ELSE h3_geo_to_h3(ST_Y(ST_Centroid(geom)), ST_X(ST_Centroid(geom)), 7)
                        END,
                        h3_res_8 = CASE 
                            WHEN ST_GeometryType(geom) = 'ST_Point' THEN h3_geo_to_h3(ST_Y(geom), ST_X(geom), 8)
                            ELSE h3_geo_to_h3(ST_Y(ST_Centroid(geom)), ST_X(ST_Centroid(geom)), 8)
                        END
                    WHERE geom IS NOT NULL;
                    ''',
                    'estimated_processing_time_hours': max(0.3, total_records / 300000)
                }
            
            # –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —Ä–µ–∑–æ–ª—é—Ü—ñ–π
            h3_plan['resolution_usage'] = {
                '7': {
                    'cell_edge_km': 5.16,
                    'area_km2': 23.24,
                    'use_case': 'Regional analysis, macro location planning'
                },
                '8': {
                    'cell_edge_km': 1.95,
                    'area_km2': 3.32,
                    'use_case': 'City district analysis, catchment areas'
                },
                '9': {
                    'cell_edge_km': 0.74,
                    'area_km2': 0.47,
                    'use_case': 'Neighborhood analysis, competition mapping'
                },
                '10': {
                    'cell_edge_km': 0.28,
                    'area_km2': 0.067,
                    'use_case': 'Micro-location analysis, site selection'
                }
            }
            
            # SQL —Ñ—É–Ω–∫—Ü—ñ—ó
            h3_plan['sql_functions_needed'] = [
                'h3_geo_to_h3(lat, lon, resolution) - Convert coordinates to H3',
                'h3_h3_to_parent(h3_index, parent_resolution) - Get parent cell',
                'h3_h3_to_children(h3_index, child_resolution) - Get child cells',
                'h3_k_ring(h3_index, k) - Get neighboring cells',
                'h3_h3_to_geo(h3_index) - Convert H3 to coordinates',
                'h3_hex_area_km2(resolution) - Get cell area',
                'h3_edge_length_km(resolution) - Get edge length'
            ]
            
            # –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
            if total_records > 1000000:
                h3_plan['performance_optimization'] = {
                    'batch_processing': True,
                    'batch_size': 50000,
                    'parallel_workers': min(8, max(2, total_records // 500000)),
                    'memory_limit_gb': max(4, total_records // 1000000),
                    'vacuum_between_batches': True
                }
            
        except Exception as e:
            h3_plan['error'] = str(e)
            
        return h3_plan
    
    def _estimate_performance(self, analysis: Dict) -> Dict[str, Any]:
        """–û—Ü—ñ–Ω–∫–∞ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ —Å–∏—Å—Ç–µ–º–∏"""
        
        performance = {
            'import_estimates': {},
            'query_performance': {},
            'storage_requirements': {},
            'scalability_assessment': {}
        }
        
        try:
            data_structure = analysis.get('data_structure', {})
            total_records = data_structure.get('total_records', 0)
            file_size_mb = analysis.get('file_info', {}).get('size_mb', 0)
            
            # –û—Ü—ñ–Ω–∫–∏ —ñ–º–ø–æ—Ä—Ç—É
            performance['import_estimates'] = {
                'gpkg_to_postgis_hours': max(0.1, file_size_mb / 1000),  # ~1GB/hour
                'h3_calculation_hours': max(0.1, total_records / 500000),  # ~500k records/hour
                'index_creation_hours': max(0.1, total_records / 1000000),  # ~1M records/hour
                'total_setup_time_hours': 0
            }
            
            performance['import_estimates']['total_setup_time_hours'] = sum([
                performance['import_estimates']['gpkg_to_postgis_hours'],
                performance['import_estimates']['h3_calculation_hours'],
                performance['import_estimates']['index_creation_hours']
            ])
            
            # –ü—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å –∑–∞–ø–∏—Ç—ñ–≤
            if total_records < 100000:
                query_performance = 'excellent'
                typical_query_ms = 50
            elif total_records < 1000000:
                query_performance = 'good'
                typical_query_ms = 200
            elif total_records < 5000000:
                query_performance = 'moderate'
                typical_query_ms = 1000
            else:
                query_performance = 'needs_optimization'
                typical_query_ms = 5000
            
            performance['query_performance'] = {
                'assessment': query_performance,
                'typical_h3_query_ms': typical_query_ms,
                'spatial_query_ms': typical_query_ms * 2,
                'full_text_search_ms': typical_query_ms * 1.5,
                'recommendations': []
            }
            
            if query_performance == 'needs_optimization':
                performance['query_performance']['recommendations'].extend([
                    'Enable partitioning',
                    'Use materialized views',
                    'Consider read replicas',
                    'Implement connection pooling'
                ])
            
            # –í–∏–º–æ–≥–∏ –¥–æ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è
            raw_data_gb = file_size_mb / 1024
            indexes_gb = raw_data_gb * 0.3  # –Ü–Ω–¥–µ–∫—Å–∏ –∑–∞–∑–≤–∏—á–∞–π 30% –≤—ñ–¥ –¥–∞–Ω–∏—Ö
            h3_overhead_gb = total_records * 4 * 15 / (1024**3)  # 4 H3 cols * 15 chars
            materialized_views_gb = raw_data_gb * 0.1  # 10% –¥–ª—è MV
            
            performance['storage_requirements'] = {
                'raw_data_gb': round(raw_data_gb, 2),
                'indexes_gb': round(indexes_gb, 2),
                'h3_overhead_gb': round(h3_overhead_gb, 2),
                'materialized_views_gb': round(materialized_views_gb, 2),
                'total_estimated_gb': round(raw_data_gb + indexes_gb + h3_overhead_gb + materialized_views_gb, 2),
                'recommended_disk_gb': round((raw_data_gb + indexes_gb + h3_overhead_gb + materialized_views_gb) * 2, 2)  # 2x –¥–ª—è –±–µ–∑–ø–µ–∫–∏
            }
            
            # –û—Ü—ñ–Ω–∫–∞ –º–∞—Å—à—Ç–∞–±–æ–≤–∞–Ω–æ—Å—Ç—ñ
            if total_records < 1000000:
                scalability = 'single_server_sufficient'
            elif total_records < 10000000:
                scalability = 'single_server_with_optimization'
            else:
                scalability = 'consider_distributed_setup'
            
            performance['scalability_assessment'] = {
                'current_data_scale': scalability,
                'all_ukraine_projection': {
                    'estimated_total_records': total_records * 24,  # 24 regions
                    'estimated_storage_gb': performance['storage_requirements']['total_estimated_gb'] * 24,
                    'recommended_architecture': 'distributed_with_sharding' if total_records > 2000000 else 'single_server_optimized'
                }
            }
            
        except Exception as e:
            performance['error'] = str(e)
            
        return performance
    
    def analyze_multiple_regions(self, target_files: List[str] = None, sample_size: int = 5000) -> Dict[str, Any]:
        """–ê–Ω–∞–ª—ñ–∑ –¥–µ–∫—ñ–ª—å–∫–æ—Ö —Ä–µ–≥—ñ–æ–Ω—ñ–≤"""
        
        logger.info("üó∫Ô∏è –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª—ñ–∑—É –¥–µ–∫—ñ–ª—å–∫–æ—Ö —Ä–µ–≥—ñ–æ–Ω—ñ–≤")
        
        # –ó–Ω–∞—Ö–æ–¥–∏–º–æ —Ñ–∞–π–ª–∏
        gpkg_files = list(self.data_directory.glob("*.gpkg"))
        
        if not gpkg_files:
            logger.error(f"‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ .gpkg —Ñ–∞–π–ª—ñ–≤ –≤ {self.data_directory}")
            return {}
        
        # –í–∏–±—ñ—Ä —Ñ–∞–π–ª—ñ–≤ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
        if target_files:
            files_to_analyze = [self.data_directory / f for f in target_files if (self.data_directory / f).exists()]
        else:
            # –ê–Ω–∞–ª—ñ–∑—É—î–º–æ 3 —Ñ–∞–π–ª–∏ —Ä—ñ–∑–Ω–∏—Ö —Ä–æ–∑–º—ñ—Ä—ñ–≤ –¥–ª—è —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω–æ—Å—Ç—ñ
            sorted_files = sorted(gpkg_files, key=lambda f: f.stat().st_size)
            files_to_analyze = [
                sorted_files[0],  # –ù–∞–π–º–µ–Ω—à–∏–π
                sorted_files[len(sorted_files)//2],  # –°–µ—Ä–µ–¥–Ω—ñ–π
                sorted_files[-1]  # –ù–∞–π–±—ñ–ª—å—à–∏–π
            ]
        
        logger.info(f"üìã –û–±—Ä–∞–Ω–æ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É: {[f.name for f in files_to_analyze]}")
        
        # –ê–Ω–∞–ª—ñ–∑ –∫–æ–∂–Ω–æ–≥–æ —Ñ–∞–π–ª—É
        regional_analyses = {}
        
        for gpkg_file in files_to_analyze:
            logger.info(f"üîç –ê–Ω–∞–ª—ñ–∑ {gpkg_file.name}...")
            regional_analysis = self.analyze_hot_osm_file(gpkg_file, sample_size)
            regional_analyses[gpkg_file.name] = regional_analysis
        
        # –ó–≤–µ–¥–µ–Ω–∏–π –∞–Ω–∞–ª—ñ–∑
        consolidated_analysis = self._create_consolidated_analysis(regional_analyses)
        
        complete_analysis = {
            'analysis_timestamp': datetime.now().isoformat(),
            'analyzed_regions': list(regional_analyses.keys()),
            'regional_details': regional_analyses,
            'consolidated_analysis': consolidated_analysis,
            'ukraine_wide_projections': self._project_ukraine_wide(consolidated_analysis),
            'implementation_roadmap': self._create_implementation_roadmap(consolidated_analysis)
        }
        
        return complete_analysis
    
    def _create_consolidated_analysis(self, regional_analyses: Dict) -> Dict[str, Any]:
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∑–≤–µ–¥–µ–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É"""
        
        consolidated = {
            'data_volume_summary': {},
            'tag_richness_comparison': {},
            'spatial_coverage': {},
            'unified_schema_recommendations': {},
            'performance_projections': {}
        }
        
        try:
            total_records = 0
            total_size_mb = 0
            all_regions_tags = {}
            spatial_bounds = []
            
            for region_name, analysis in regional_analyses.items():
                # –û–±—Å—è–≥ –¥–∞–Ω–∏—Ö
                data_structure = analysis.get('data_structure', {})
                file_info = analysis.get('file_info', {})
                
                region_records = data_structure.get('total_records', 0)
                region_size = file_info.get('size_mb', 0)
                
                total_records += region_records
                total_size_mb += region_size
                
                # –¢–µ–≥–∏
                tag_analysis = analysis.get('tag_analysis', {})
                key_retail_tags = tag_analysis.get('key_retail_tags', {})
                all_regions_tags[region_name] = key_retail_tags
                
                # –ü—Ä–æ—Å—Ç–æ—Ä–æ–≤–µ –ø–æ–∫—Ä–∏—Ç—Ç—è
                spatial_analysis = analysis.get('spatial_analysis', {})
                extent = spatial_analysis.get('spatial_extent', {})
                if 'bounds' in extent:
                    spatial_bounds.append(extent['bounds'])
            
            # –ó–≤–µ–¥–µ–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            consolidated['data_volume_summary'] = {
                'total_records_analyzed': total_records,
                'total_size_mb_analyzed': total_size_mb,
                'average_records_per_region': total_records // len(regional_analyses) if regional_analyses else 0,
                'projected_all_ukraine_records': total_records * (24 // len(regional_analyses)) if regional_analyses else 0,
                'projected_all_ukraine_size_gb': (total_size_mb * (24 // len(regional_analyses))) / 1024 if regional_analyses else 0
            }
            
            # –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –±–∞–≥–∞—Ç—Å—Ç–≤–∞ —Ç–µ–≥—ñ–≤
            tag_consistency = self._analyze_tag_consistency(all_regions_tags)
            consolidated['tag_richness_comparison'] = tag_consistency
            
            # –ü—Ä–æ—Å—Ç–æ—Ä–æ–≤–µ –ø–æ–∫—Ä–∏—Ç—Ç—è
            if spatial_bounds:
                overall_bounds = {
                    'minx': min(b['minx'] for b in spatial_bounds),
                    'miny': min(b['miny'] for b in spatial_bounds),
                    'maxx': max(b['maxx'] for b in spatial_bounds),
                    'maxy': max(b['maxy'] for b in spatial_bounds)
                }
                
                consolidated['spatial_coverage'] = {
                    'analyzed_regions_bounds': overall_bounds,
                    'coverage_area_km2': self._calculate_approximate_area(overall_bounds),
                    'regions_analyzed': len(regional_analyses)
                }
            
            # –£–Ω—ñ—Ñ—ñ–∫–æ–≤–∞–Ω–∞ —Å—Ö–µ–º–∞
            consolidated['unified_schema_recommendations'] = self._create_unified_schema(regional_analyses)
            
        except Exception as e:
            consolidated['error'] = str(e)
            
        return consolidated
    
    def _analyze_tag_consistency(self, all_regions_tags: Dict) -> Dict[str, Any]:
        """–ê–Ω–∞–ª—ñ–∑ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—ñ —Ç–µ–≥—ñ–≤ –º—ñ–∂ —Ä–µ–≥—ñ–æ–Ω–∞–º–∏"""
        
        consistency = {
            'common_tags_across_regions': {},
            'region_specific_tags': {},
            'tag_coverage_variance': {}
        }
        
        try:
            # –ó–±—ñ—Ä –≤—Å—ñ—Ö —Ç–µ–≥—ñ–≤
            all_possible_tags = set()
            for region_tags in all_regions_tags.values():
                all_possible_tags.update(region_tags.keys())
            
            # –ê–Ω–∞–ª—ñ–∑ –ø–æ—à–∏—Ä–µ–Ω–æ—Å—Ç—ñ —Ç–µ–≥—ñ–≤
            for tag in all_possible_tags:
                regions_with_tag = []
                coverage_rates = []
                
                for region_name, region_tags in all_regions_tags.items():
                    if tag in region_tags:
                        regions_with_tag.append(region_name)
                        coverage_rates.append(region_tags[tag].get('occurrence_rate', 0))
                
                consistency['common_tags_across_regions'][tag] = {
                    'present_in_regions': len(regions_with_tag),
                    'coverage_variance': float(np.var(coverage_rates)) if coverage_rates else 0,
                    'average_coverage': float(np.mean(coverage_rates)) if coverage_rates else 0
                }
            
            # –¢–æ–ø –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ñ —Ç–µ–≥–∏
            consistent_tags = sorted(
                consistency['common_tags_across_regions'].items(),
                key=lambda x: (x[1]['present_in_regions'], -x[1]['coverage_variance']),
                reverse=True
            )
            
            consistency['most_consistent_tags'] = dict(consistent_tags[:10])
            
        except Exception:
            pass
            
        return consistency
    
    def _calculate_approximate_area(self, bounds: Dict) -> float:
        """–ü—Ä–∏–±–ª–∏–∑–Ω–∏–π —Ä–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –ø–ª–æ—â—ñ –≤ –∫–º¬≤"""
        try:
            width_deg = bounds['maxx'] - bounds['minx']
            height_deg = bounds['maxy'] - bounds['miny']
            
            # –î–ª—è –£–∫—Ä–∞—ó–Ω–∏ (–ø—Ä–∏–±–ª–∏–∑–Ω–æ 50¬∞ –ø—ñ–≤–Ω—ñ—á–Ω–æ—ó —à–∏—Ä–æ—Ç–∏)
            lat_center = (bounds['miny'] + bounds['maxy']) / 2
            km_per_deg_lat = 111.0
            km_per_deg_lon = 111.0 * np.cos(np.radians(lat_center))
            
            area_km2 = width_deg * km_per_deg_lon * height_deg * km_per_deg_lat
            return round(area_km2, 2)
        except:
            return 0.0
    
    def _create_unified_schema(self, regional_analyses: Dict) -> Dict[str, Any]:
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è —É–Ω—ñ—Ñ—ñ–∫–æ–≤–∞–Ω–æ—ó —Å—Ö–µ–º–∏ –¥–ª—è –≤—Å—ñ—Ö —Ä–µ–≥—ñ–æ–Ω—ñ–≤"""
        
        unified_schema = {
            'base_table_structure': {},
            'region_specific_adaptations': {},
            'global_indexes': [],
            'federation_strategy': {}
        }
        
        try:
            # –ë–∞–∑–æ–≤–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ (–Ω–∞–π–±—ñ–ª—å—à –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–∞)
            unified_schema['base_table_structure'] = {
                'table_name': 'osm_ukraine_unified',
                'columns': [
                    'id BIGSERIAL PRIMARY KEY',
                    'region_name VARCHAR(50) NOT NULL',
                    'original_fid INTEGER',
                    'osm_id BIGINT',
                    'osm_type VARCHAR(20)',
                    'version INTEGER',
                    'changeset INTEGER',
                    'uid INTEGER',
                    'username VARCHAR(255)',
                    'timestamp TIMESTAMP WITH TIME ZONE',
                    'geom GEOMETRY(GEOMETRY, 4326)',
                    'tags JSONB',
                    'h3_res_7 VARCHAR(15)',
                    'h3_res_8 VARCHAR(15)',
                    'h3_res_9 VARCHAR(15)',
                    'h3_res_10 VARCHAR(15)',
                    'created_at TIMESTAMP DEFAULT NOW()',
                    'updated_at TIMESTAMP DEFAULT NOW()'
                ],
                'partitioning': {
                    'method': 'PARTITION BY LIST (region_name)',
                    'benefits': ['Parallel queries', 'Regional data isolation', 'Easier maintenance']
                }
            }
            
            # –ì–ª–æ–±–∞–ª—å–Ω—ñ —ñ–Ω–¥–µ–∫—Å–∏
            unified_schema['global_indexes'] = [
                'CREATE INDEX idx_osm_ukraine_geom ON osm_ukraine_unified USING GIST (geom)',
                'CREATE INDEX idx_osm_ukraine_h3_8 ON osm_ukraine_unified (h3_res_8)',
                'CREATE INDEX idx_osm_ukraine_h3_9 ON osm_ukraine_unified (h3_res_9)',
                'CREATE INDEX idx_osm_ukraine_tags_gin ON osm_ukraine_unified USING GIN (tags)',
                'CREATE INDEX idx_osm_ukraine_region ON osm_ukraine_unified (region_name)',
                'CREATE INDEX idx_osm_ukraine_osm_id ON osm_ukraine_unified (osm_id)',
                'CREATE INDEX idx_osm_ukraine_poi ON osm_ukraine_unified USING GIN ((tags->\'amenity\'), (tags->\'shop\'))'
            ]
            
            # –°—Ç—Ä–∞—Ç–µ–≥—ñ—è —Ñ–µ–¥–µ—Ä–∞—Ü—ñ—ó
            unified_schema['federation_strategy'] = {
                'approach': 'single_database_partitioned',
                'alternative': 'federated_databases_per_region',
                'recommended': 'single_database_partitioned',
                'reasoning': 'Easier cross-region analytics, unified H3 grid, simpler maintenance'
            }
            
        except Exception as e:
            unified_schema['error'] = str(e)
            
        return unified_schema
    
    def _project_ukraine_wide(self, consolidated_analysis: Dict) -> Dict[str, Any]:
        """–ü—Ä–æ–µ–∫—Ü—ñ—è –Ω–∞ –≤—Å—é –£–∫—Ä–∞—ó–Ω—É"""
        
        ukraine_projection = {
            'data_volume_projections': {},
            'infrastructure_requirements': {},
            'implementation_phases': {},
            'cost_estimates': {}
        }
        
        try:
            data_summary = consolidated_analysis.get('data_volume_summary', {})
            
            # –ü—Ä–æ–µ–∫—Ü—ñ—è –æ–±—Å—è–≥—É –¥–∞–Ω–∏—Ö
            ukraine_projection['data_volume_projections'] = {
                'total_records_estimate': data_summary.get('projected_all_ukraine_records', 0),
                'total_storage_gb_estimate': data_summary.get('projected_all_ukraine_size_gb', 0),
                'daily_update_volume_mb': data_summary.get('projected_all_ukraine_size_gb', 0) * 1024 * 0.01,  # 1% daily changes
                'h3_index_storage_gb': data_summary.get('projected_all_ukraine_records', 0) * 4 * 15 / (1024**3),
                'materialized_views_gb': data_summary.get('projected_all_ukraine_size_gb', 0) * 0.15
            }
            
            total_storage_needed = (
                ukraine_projection['data_volume_projections']['total_storage_gb_estimate'] * 2.5  # Raw data + indexes + overhead
            )
            
            # –í–∏–º–æ–≥–∏ –¥–æ —ñ–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∏
            ukraine_projection['infrastructure_requirements'] = {
                'database_server': {
                    'cpu_cores': max(16, total_storage_needed // 100),
                    'ram_gb': max(64, total_storage_needed // 10),
                    'storage_gb': total_storage_needed * 1.5,  # –ó –∑–∞–ø–∞—Å–æ–º
                    'storage_type': 'NVMe SSD –¥–ª—è optimal performance',
                    'network_gbps': 10
                },
                'application_servers': {
                    'count': max(2, total_storage_needed // 500),
                    'cpu_cores_each': 8,
                    'ram_gb_each': 32
                },
                'backup_requirements': {
                    'storage_gb': total_storage_needed * 1.2,
                    'backup_frequency': 'daily_incremental',
                    'retention_days': 30
                }
            }
            
            # –§–∞–∑–∏ –≤–ø—Ä–æ–≤–∞–¥–∂–µ–Ω–Ω—è
            ukraine_projection['implementation_phases'] = {
                'phase_1_pilot': {
                    'regions': ['Kyiv', 'Lviv', 'Kharkiv'],
                    'duration_weeks': 4,
                    'goals': ['Schema validation', 'Performance testing', 'H3 integration testing']
                },
                'phase_2_expansion': {
                    'regions': ['All major cities'],
                    'duration_weeks': 8,
                    'goals': ['Scale testing', 'ETL pipeline optimization', 'API development']
                },
                'phase_3_complete': {
                    'regions': ['All 24 regions'],
                    'duration_weeks': 6,
                    'goals': ['Full deployment', 'Monitoring setup', 'Documentation']
                }
            }
            
            # –ü—Ä–∏–±–ª–∏–∑–Ω—ñ –æ—Ü—ñ–Ω–∫–∏ –≤–∏—Ç—Ä–∞—Ç
            ukraine_projection['cost_estimates'] = {
                'infrastructure_monthly_usd': {
                    'database_server': max(500, total_storage_needed * 2),
                    'application_servers': ukraine_projection['infrastructure_requirements']['application_servers']['count'] * 200,
                    'storage_backup': total_storage_needed * 0.5,
                    'network_bandwidth': 200,
                    'monitoring_tools': 100
                },
                'development_costs_usd': {
                    'etl_pipeline_development': 15000,
                    'api_development': 25000,
                    'frontend_dashboard': 20000,
                    'testing_qa': 10000,
                    'documentation': 5000
                },
                'operational_monthly_usd': {
                    'devops_engineer_partial': 2000,
                    'data_engineer_partial': 1500,
                    'monitoring_maintenance': 500
                }
            }
            
        except Exception as e:
            ukraine_projection['error'] = str(e)
            
        return ukraine_projection
    
    def _create_implementation_roadmap(self, consolidated_analysis: Dict) -> Dict[str, Any]:
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–æ—Ä–æ–∂–Ω—å–æ—ó –∫–∞—Ä—Ç–∏ –≤–ø—Ä–æ–≤–∞–¥–∂–µ–Ω–Ω—è"""
        
        roadmap = {
            'immediate_next_steps': [],
            'technical_milestones': {},
            'risk_mitigation': {},
            'success_metrics': {}
        }
        
        try:
            # –ù–µ–≥–∞–π–Ω—ñ –Ω–∞—Å—Ç—É–ø–Ω—ñ –∫—Ä–æ–∫–∏
            roadmap['immediate_next_steps'] = [
                {
                    'step': 'Setup PostGIS environment',
                    'duration_days': 2,
                    'requirements': ['PostgreSQL 15+', 'PostGIS 3.4+', 'H3 extension'],
                    'deliverable': 'Working PostGIS instance with H3 support'
                },
                {
                    'step': 'Implement pilot ETL pipeline',
                    'duration_days': 5,
                    'requirements': ['Python environment', 'GeoPandas', 'SQLAlchemy'],
                    'deliverable': 'Working ETL for 1-2 regions'
                },
                {
                    'step': 'Create unified schema',
                    'duration_days': 3,
                    'requirements': ['Schema analysis results', 'H3 integration plan'],
                    'deliverable': 'Production-ready database schema'
                },
                {
                    'step': 'Develop H3 processing functions',
                    'duration_days': 4,
                    'requirements': ['H3 PostGIS extension', 'Batch processing logic'],
                    'deliverable': 'Automated H3 indexing pipeline'
                },
                {
                    'step': 'Performance testing',
                    'duration_days': 3,
                    'requirements': ['Sample data loaded', 'Query benchmark suite'],
                    'deliverable': 'Performance baseline and optimization recommendations'
                }
            ]
            
            # –¢–µ—Ö–Ω—ñ—á–Ω—ñ –≤—ñ—Ö–∏
            roadmap['technical_milestones'] = {
                'week_2': {
                    'milestone': 'Single region successfully imported',
                    'criteria': ['All data imported', 'H3 indexing complete', 'Basic queries working']
                },
                'week_4': {
                    'milestone': 'Multi-region federation working',
                    'criteria': ['3+ regions loaded', 'Cross-region queries', 'Performance acceptable']
                },
                'week_8': {
                    'milestone': 'API and analytics ready',
                    'criteria': ['REST API functional', 'Basic analytics working', 'Dashboard prototype']
                },
                'week_12': {
                    'milestone': 'Production ready system',
                    'criteria': ['All regions loaded', 'Monitoring in place', 'Documentation complete']
                }
            }
            
            # –†–∏–∑–∏–∫–∏ —Ç–∞ –º—ñ—Ç–∏–≥–∞—Ü—ñ—è
            roadmap['risk_mitigation'] = {
                'data_quality_issues': {
                    'risk': 'Inconsistent OSM data across regions',
                    'probability': 'medium',
                    'impact': 'medium',
                    'mitigation': 'Robust error handling, data validation pipelines, fallback strategies'
                },
                'performance_bottlenecks': {
                    'risk': 'Slow queries on large datasets',
                    'probability': 'high',
                    'impact': 'high',
                    'mitigation': 'Partitioning, materialized views, query optimization, caching'
                },
                'h3_integration_complexity': {
                    'risk': 'H3 extension issues or performance problems',
                    'probability': 'low',
                    'impact': 'high',
                    'mitigation': 'Fallback to custom H3 implementation, pre-testing, alternative indexing'
                },
                'storage_costs': {
                    'risk': 'Higher than expected storage requirements',
                    'probability': 'medium',
                    'impact': 'medium',
                    'mitigation': 'Data compression, archiving strategies, cloud auto-scaling'
                }
            }
            
            # –ú–µ—Ç—Ä–∏–∫–∏ —É—Å–ø—ñ—Ö—É
            roadmap['success_metrics'] = {
                'data_metrics': {
                    'data_coverage': '>95% of Ukraine territory',
                    'data_freshness': '<30 days old',
                    'data_quality': '>90% valid geometries'
                },
                'performance_metrics': {
                    'query_response_time': '<500ms for H3 queries',
                    'api_availability': '>99.5%',
                    'concurrent_users': '>50 simultaneous'
                },
                'business_metrics': {
                    'location_analysis_time': '<1 hour vs 1 day manual',
                    'analysis_accuracy': '>85% vs manual methods',
                    'user_satisfaction': '>4.5/5'
                }
            }
            
        except Exception as e:
            roadmap['error'] = str(e)
            
        return roadmap
    
    def save_analysis_report(self, analysis_results: Dict, output_path: str = None):
        """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∑–≤—ñ—Ç—É –∞–Ω–∞–ª—ñ–∑—É"""
        
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"hot_osm_corrected_analysis_{timestamp}.json"
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(analysis_results, f, ensure_ascii=False, indent=2, default=str)
            
            logger.info(f"‚úÖ –ó–≤—ñ—Ç –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {output_path}")
            
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∑–≤—ñ—Ç—É: {e}")
    
    def print_analysis_summary(self, analysis_results: Dict):
        """–í–∏–≤–µ–¥–µ–Ω–Ω—è –∫–æ—Ä–æ—Ç–∫–æ–≥–æ –∑–≤—ñ—Ç—É –∞–Ω–∞–ª—ñ–∑—É"""
        
        print("\n" + "="*100)
        print("üéØ CORRECTED HOT OSM ANALYSIS - SUMMARY")
        print("="*100)
        
        if 'regional_details' in analysis_results:
            # Multi-region analysis
            self._print_multi_region_summary(analysis_results)
        else:
            # Single region analysis
            self._print_single_region_summary(analysis_results)
        
        print("="*100 + "\n")
    
    def _print_single_region_summary(self, analysis: Dict):
        """–í–∏–≤–µ–¥–µ–Ω–Ω—è –ø—ñ–¥—Å—É–º–∫—É –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ä–µ–≥—ñ–æ–Ω—É"""
        
        file_info = analysis.get('file_info', {})
        data_structure = analysis.get('data_structure', {})
        osm_content = analysis.get('osm_content_analysis', {})
        tag_analysis = analysis.get('tag_analysis', {})
        spatial_analysis = analysis.get('spatial_analysis', {})
        
        print(f"üìÅ –†–µ–≥—ñ–æ–Ω: {file_info.get('region_name', 'Unknown')}")
        print(f"üìä –†–æ–∑–º—ñ—Ä —Ñ–∞–π–ª—É: {file_info.get('size_mb', 0):.1f} MB")
        print(f"üî¢ –í—Å—å–æ–≥–æ –∑–∞–ø–∏—Å—ñ–≤: {data_structure.get('total_records', 0):,}")
        
        # OSM –∫–æ–Ω—Ç–µ–Ω—Ç
        if osm_content:
            osm_type_dist = osm_content.get('osm_type_distribution', {})
            types = osm_type_dist.get('types', {})
            print(f"\nüìã OSM –¢–ò–ü–ò:")
            for osm_type, count in types.items():
                print(f"  ‚Ä¢ {osm_type}: {count:,}")
        
        # –¢–µ–≥–∏
        if tag_analysis:
            retail_tags = tag_analysis.get('key_retail_tags', {})
            retail_relevance = tag_analysis.get('retail_relevance', {})
            
            print(f"\nüè∑Ô∏è –ö–õ–Æ–ß–û–í–Ü –¢–ï–ì–ò –î–õ–Ø –†–ï–¢–ï–ô–õ–£:")
            for tag, info in list(retail_tags.items())[:5]:
                occurrence_rate = info.get('occurrence_rate', 0)
                print(f"  ‚Ä¢ {tag}: {occurrence_rate:.1%} –ø–æ–∫—Ä–∏—Ç—Ç—è")
            
            relevance_score = retail_relevance.get('overall_score', 0)
            suitability = retail_relevance.get('suitability_assessment', 'unknown')
            print(f"\nüéØ –ü–†–ò–î–ê–¢–ù–Ü–°–¢–¨ –î–õ–Ø –†–ï–¢–ï–ô–õ–£: {suitability.upper()} (Score: {relevance_score:.2f})")
        
        # –ü—Ä–æ—Å—Ç–æ—Ä–æ–≤—ñ –¥–∞–Ω—ñ
        if spatial_analysis:
            extent = spatial_analysis.get('spatial_extent', {})
            density = spatial_analysis.get('density_analysis', {})
            
            area_km2 = extent.get('approximate_area_km2', 0)
            feature_density = density.get('features_per_km2', 0)
            
            print(f"\nüó∫Ô∏è –ü–†–û–°–¢–û–†–û–í–ï –ü–û–ö–†–ò–¢–¢–Ø:")
            print(f"  ‚Ä¢ –ü–ª–æ—â–∞: ~{area_km2:,.0f} –∫–º¬≤")
            print(f"  ‚Ä¢ –©—ñ–ª—å–Ω—ñ—Å—Ç—å –æ–±'—î–∫—Ç—ñ–≤: {feature_density:.1f} –æ–±'—î–∫—Ç—ñ–≤/–∫–º¬≤")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó
        schema_recs = analysis.get('postgis_schema_recommendations', {})
        if schema_recs:
            main_table = schema_recs.get('main_table', {})
            estimated_size = main_table.get('estimated_size_gb', 0)
            print(f"\nüíæ –û–¶–Ü–ù–ö–ê –†–û–ó–ú–Ü–†–£ –í POSTGIS: ~{estimated_size:.1f} GB")
    
    def _print_multi_region_summary(self, analysis_results: Dict):
        """–í–∏–≤–µ–¥–µ–Ω–Ω—è –ø—ñ–¥—Å—É–º–∫—É –¥–ª—è –¥–µ–∫—ñ–ª—å–∫–æ—Ö —Ä–µ–≥—ñ–æ–Ω—ñ–≤"""
        
        consolidated = analysis_results.get('consolidated_analysis', {})
        ukraine_projection = analysis_results.get('ukraine_wide_projections', {})
        
        # –ó–∞–≥–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        data_summary = consolidated.get('data_volume_summary', {})
        print(f"üìä –ü–†–û–ê–ù–ê–õ–Ü–ó–û–í–ê–ù–û –†–ï–ì–Ü–û–ù–Ü–í: {len(analysis_results.get('analyzed_regions', []))}")
        print(f"üî¢ –ó–ê–ì–ê–õ–û–ú –ó–ê–ü–ò–°–Ü–í: {data_summary.get('total_records_analyzed', 0):,}")
        print(f"üíæ –ó–ê–ì–ê–õ–¨–ù–ò–ô –†–û–ó–ú–Ü–†: {data_summary.get('total_size_mb_analyzed', 0):,.1f} MB")
        
        # –ü—Ä–æ–µ–∫—Ü—ñ—è –Ω–∞ –£–∫—Ä–∞—ó–Ω—É
        if ukraine_projection:
            volume_proj = ukraine_projection.get('data_volume_projections', {})
            infra_req = ukraine_projection.get('infrastructure_requirements', {})
            
            print(f"\nüá∫üá¶ –ü–†–û–ï–ö–¶–Ü–Ø –ù–ê –í–°–Æ –£–ö–†–ê–á–ù–£:")
            print(f"  ‚Ä¢ –û—á—ñ–∫—É–≤–∞–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–∞–ø–∏—Å—ñ–≤: {volume_proj.get('total_records_estimate', 0):,}")
            print(f"  ‚Ä¢ –û—á—ñ–∫—É–≤–∞–Ω–∏–π —Ä–æ–∑–º—ñ—Ä: {volume_proj.get('total_storage_gb_estimate', 0):,.0f} GB")
            
            db_server = infra_req.get('database_server', {})
            print(f"\nüñ•Ô∏è –†–ï–ö–û–ú–ï–ù–î–û–í–ê–ù–ê –Ü–ù–§–†–ê–°–¢–†–£–ö–¢–£–†–ê:")
            print(f"  ‚Ä¢ CPU cores: {db_server.get('cpu_cores', 0)}")
            print(f"  ‚Ä¢ RAM: {db_server.get('ram_gb', 0)} GB")
            print(f"  ‚Ä¢ Storage: {db_server.get('storage_gb', 0):,.0f} GB")
        
        # –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ñ—Å—Ç—å —Ç–µ–≥—ñ–≤
        tag_richness = consolidated.get('tag_richness_comparison', {})
        if tag_richness:
            consistent_tags = tag_richness.get('most_consistent_tags', {})
            print(f"\nüè∑Ô∏è –ù–ê–ô–ë–Ü–õ–¨–® –ö–û–ù–°–ò–°–¢–ï–ù–¢–ù–Ü –¢–ï–ì–ò:")
            for tag, info in list(consistent_tags.items())[:5]:
                regions_count = info.get('present_in_regions', 0)
                avg_coverage = info.get('average_coverage', 0)
                print(f"  ‚Ä¢ {tag}: {regions_count} —Ä–µ–≥—ñ–æ–Ω—ñ–≤, {avg_coverage:.1%} –ø–æ–∫—Ä–∏—Ç—Ç—è")
        
        # –ù–∞—Å—Ç—É–ø–Ω—ñ –∫—Ä–æ–∫–∏
        roadmap = analysis_results.get('implementation_roadmap', {})
        if roadmap:
            next_steps = roadmap.get('immediate_next_steps', [])
            print(f"\nüéØ –ù–ê–°–¢–£–ü–ù–Ü –ö–†–û–ö–ò:")
            for i, step in enumerate(next_steps[:3], 1):
                step_name = step.get('step', 'Unknown')
                duration = step.get('duration_days', 0)
                print(f"  {i}. {step_name} ({duration} –¥–Ω—ñ–≤)")


def main():
    """–ó–∞–ø—É—Å–∫ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É HOT OSM –µ–∫—Å–ø–æ—Ä—Ç—ñ–≤"""
    
    data_directory = r"C:\OSMData"
    
    print("üöÄ –ó–∞–ø—É—Å–∫ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–æ–≥–æ HOT OSM –∞–Ω–∞–ª—ñ–∑—É...")
    print(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è: {data_directory}")
    
    analyzer = CorrectedHOTOSMAnalyzer(data_directory)
    
    try:
        # –í–∏–±—ñ—Ä —Ç–∏–ø—É –∞–Ω–∞–ª—ñ–∑—É
        analysis_type = input("\n–û–±–µ—Ä—ñ—Ç—å —Ç–∏–ø –∞–Ω–∞–ª—ñ–∑—É:\n1. –û–¥–∏–Ω —Ñ–∞–π–ª (—à–≤–∏–¥–∫–æ)\n2. –î–µ–∫—ñ–ª—å–∫–∞ —Ñ–∞–π–ª—ñ–≤ (–ø–æ–≤–Ω–∏–π)\n–í–∞—à –≤–∏–±—ñ—Ä (1/2): ").strip()
        
        if analysis_type == "1":
            # –ê–Ω–∞–ª—ñ–∑ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª—É
            gpkg_files = list(Path(data_directory).glob("*.gpkg"))
            if not gpkg_files:
                print("‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ .gpkg —Ñ–∞–π–ª—ñ–≤")
                return
            
            # –í–∏–±–∏—Ä–∞—î–º–æ –Ω–∞–π–º–µ–Ω—à–∏–π –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ
            selected_file = min(gpkg_files, key=lambda f: f.stat().st_size)
            print(f"üìã –û–±—Ä–∞–Ω–æ: {selected_file.name}")
            
            analysis_results = analyzer.analyze_hot_osm_file(selected_file)
            
        else:
            # –ê–Ω–∞–ª—ñ–∑ –¥–µ–∫—ñ–ª—å–∫–æ—Ö —Ñ–∞–π–ª—ñ–≤
            analysis_results = analyzer.analyze_multiple_regions()
        
        if analysis_results:
            # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∑–≤—ñ—Ç—É
            analyzer.save_analysis_report(analysis_results)
            
            # –í–∏–≤–µ–¥–µ–Ω–Ω—è –ø—ñ–¥—Å—É–º–∫—É
            analyzer.print_analysis_summary(analysis_results)
            
            print(f"\n‚úÖ –ê–Ω–∞–ª—ñ–∑ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ!")
        else:
            print(f"‚ùå –ê–Ω–∞–ª—ñ–∑ –Ω–µ –¥–∞–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤")
            
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∞–Ω–∞–ª—ñ–∑—É: {e}")
        logger.exception("–î–µ—Ç–∞–ª—å–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –ø–æ–º–∏–ª–∫—É:")


if __name__ == "__main__":
    main()