#!/usr/bin/env python3
"""
OSM HOT Export Diagnostic Analyzer
–°–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π –∞–Ω–∞–ª—ñ–∑–∞—Ç–æ—Ä –¥–ª—è –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ HOT OSM –µ–∫—Å–ø–æ—Ä—Ç—ñ–≤ (.gpkg)
"""

import geopandas as gpd
import pandas as pd
import json
import sqlite3
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import logging
from collections import defaultdict, Counter
import numpy as np
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class HOTOSMDiagnosticAnalyzer:
    """–î—ñ–∞–≥–Ω–æ—Å—Ç–∏—á–Ω–∏–π –∞–Ω–∞–ª—ñ–∑–∞—Ç–æ—Ä —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ HOT OSM –µ–∫—Å–ø–æ—Ä—Ç—ñ–≤"""
    
    def __init__(self, data_directory: str):
        self.data_directory = Path(data_directory)
        self.analysis_results = {}
        
    def diagnose_single_file(self, gpkg_path: Path, max_sample_size: int = 10) -> Dict[str, Any]:
        """–ü–æ–≤–Ω–∞ –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –æ–¥–Ω–æ–≥–æ .gpkg —Ñ–∞–π–ª—É"""
        
        logger.info(f"üîç –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Ñ–∞–π–ª—É: {gpkg_path.name}")
        
        file_analysis = {
            'file_info': {
                'filename': gpkg_path.name,
                'size_mb': round(gpkg_path.stat().st_size / (1024 * 1024), 2),
                'full_path': str(gpkg_path)
            },
            'sqlite_structure': {},
            'layers_detailed': {},
            'osm_tag_analysis': {},
            'geometry_analysis': {},
            'recommendations': {}
        }
        
        try:
            # 1. –ê–Ω–∞–ª—ñ–∑ SQLite —Å—Ç—Ä—É–∫—Ç—É—Ä–∏
            file_analysis['sqlite_structure'] = self._analyze_sqlite_structure(gpkg_path)
            
            # 2. –ê–Ω–∞–ª—ñ–∑ —à–∞—Ä—ñ–≤
            layers_info = gpd.list_layers(gpkg_path)
            logger.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ —à–∞—Ä—ñ–≤: {len(layers_info)}")
            
            for layer_info in layers_info:
                layer_name = layer_info[0] if isinstance(layer_info, tuple) else layer_info
                logger.info(f"  üìä –ê–Ω–∞–ª—ñ–∑ —à–∞—Ä—É: {layer_name}")
                
                layer_analysis = self._diagnose_layer_comprehensive(
                    gpkg_path, layer_name, max_sample_size
                )
                file_analysis['layers_detailed'][layer_name] = layer_analysis
            
            # 3. –ì–ª–æ–±–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ OSM —Ç–µ–≥—ñ–≤
            file_analysis['osm_tag_analysis'] = self._analyze_osm_tags_globally(
                file_analysis['layers_detailed']
            )
            
            # 4. –ê–Ω–∞–ª—ñ–∑ –≥–µ–æ–º–µ—Ç—Ä—ñ–π
            file_analysis['geometry_analysis'] = self._analyze_geometries_globally(
                file_analysis['layers_detailed']
            )
            
            # 5. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –¥–ª—è PostGIS —Å—Ö–µ–º–∏
            file_analysis['recommendations'] = self._generate_schema_recommendations(
                file_analysis
            )
            
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏: {e}")
            file_analysis['error'] = str(e)
            
        return file_analysis
    
    def _analyze_sqlite_structure(self, gpkg_path: Path) -> Dict[str, Any]:
        """–ê–Ω–∞–ª—ñ–∑ –≤–Ω—É—Ç—Ä—ñ—à–Ω—å–æ—ó —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ SQLite/GPKG"""
        
        structure = {
            'tables': [],
            'indexes': [],
            'triggers': [],
            'views': [],
            'metadata': {}
        }
        
        try:
            with sqlite3.connect(gpkg_path) as conn:
                cursor = conn.cursor()
                
                # –û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å–ø–∏—Å–∫—É —Ç–∞–±–ª–∏—Ü—å
                cursor.execute("""
                    SELECT name, type, sql 
                    FROM sqlite_master 
                    WHERE type IN ('table', 'index', 'trigger', 'view')
                    ORDER BY type, name
                """)
                
                for name, obj_type, sql in cursor.fetchall():
                    if obj_type == 'table':
                        # –ê–Ω–∞–ª—ñ–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ —Ç–∞–±–ª–∏—Ü—ñ
                        cursor.execute(f"PRAGMA table_info('{name}')")
                        columns = cursor.fetchall()
                        
                        cursor.execute(f"SELECT COUNT(*) FROM '{name}'")
                        row_count = cursor.fetchone()[0]
                        
                        structure['tables'].append({
                            'name': name,
                            'row_count': row_count,
                            'columns': [
                                {
                                    'name': col[1], 
                                    'type': col[2], 
                                    'nullable': not col[3],
                                    'primary_key': bool(col[5])
                                } 
                                for col in columns
                            ],
                            'create_sql': sql
                        })
                    elif obj_type == 'index':
                        structure['indexes'].append({'name': name, 'sql': sql})
                    elif obj_type == 'trigger':
                        structure['triggers'].append({'name': name, 'sql': sql})
                    elif obj_type == 'view':
                        structure['views'].append({'name': name, 'sql': sql})
                
                # GPKG –º–µ—Ç–∞–¥–∞–Ω—ñ
                cursor.execute("""
                    SELECT table_name, data_type, identifier, description, srs_id
                    FROM gpkg_contents
                """)
                
                gpkg_contents = cursor.fetchall()
                structure['metadata']['gpkg_contents'] = [
                    {
                        'table_name': row[0],
                        'data_type': row[1], 
                        'identifier': row[2],
                        'description': row[3],
                        'srs_id': row[4]
                    }
                    for row in gpkg_contents
                ]
                
        except Exception as e:
            logger.warning(f"–ü–æ–º–∏–ª–∫–∞ –∞–Ω–∞–ª—ñ–∑—É SQLite —Å—Ç—Ä—É–∫—Ç—É—Ä–∏: {e}")
            structure['error'] = str(e)
            
        return structure
    
    def _diagnose_layer_comprehensive(self, gpkg_path: Path, layer_name: str, 
                                    max_sample_size: int) -> Dict[str, Any]:
        """–ü–æ–≤–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ –æ–∫—Ä–µ–º–æ–≥–æ —à–∞—Ä—É"""
        
        layer_diagnosis = {
            'layer_name': layer_name,
            'basic_info': {},
            'geometry_details': {},
            'attributes_analysis': {},
            'osm_tags_analysis': {},
            'sample_data': {},
            'data_quality': {},
            'h3_suitability': {}
        }
        
        try:
            # –ß–∏—Ç–∞–Ω–Ω—è —à–∞—Ä—É –∑ –æ–±–º–µ–∂–µ–Ω–Ω—è–º –ø–∞–º'—è—Ç—ñ
            gdf = gpd.read_file(gpkg_path, layer=layer_name)
            
            if gdf.empty:
                layer_diagnosis['basic_info']['empty'] = True
                return layer_diagnosis
            
            # –ë–∞–∑–æ–≤–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è
            layer_diagnosis['basic_info'] = {
                'total_features': len(gdf),
                'columns_count': len(gdf.columns),
                'memory_usage_mb': round(gdf.memory_usage(deep=True).sum() / (1024*1024), 2),
                'crs': str(gdf.crs) if gdf.crs else None,
                'bounds': list(gdf.total_bounds) if not gdf.empty else None
            }
            
            # –î–µ—Ç–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ –≥–µ–æ–º–µ—Ç—Ä—ñ—ó
            layer_diagnosis['geometry_details'] = self._analyze_layer_geometry(gdf)
            
            # –ê–Ω–∞–ª—ñ–∑ –∞—Ç—Ä–∏–±—É—Ç—ñ–≤
            layer_diagnosis['attributes_analysis'] = self._analyze_layer_attributes(gdf)
            
            # –°–ø–µ—Ü–∏—Ñ—ñ—á–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ OSM —Ç–µ–≥—ñ–≤
            layer_diagnosis['osm_tags_analysis'] = self._analyze_osm_tags_in_layer(gdf)
            
            # –í–∏–±—ñ—Ä–∫–∞ –ø—Ä–∏–∫–ª–∞–¥—ñ–≤
            layer_diagnosis['sample_data'] = self._extract_representative_samples(
                gdf, max_sample_size
            )
            
            # –û—Ü—ñ–Ω–∫–∞ —è–∫–æ—Å—Ç—ñ –¥–∞–Ω–∏—Ö
            layer_diagnosis['data_quality'] = self._assess_layer_quality(gdf)
            
            # –û—Ü—ñ–Ω–∫–∞ –ø—Ä–∏–¥–∞—Ç–Ω–æ—Å—Ç—ñ –¥–ª—è H3
            layer_diagnosis['h3_suitability'] = self._assess_h3_suitability(gdf)
            
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –∞–Ω–∞–ª—ñ–∑—É —à–∞—Ä—É {layer_name}: {e}")
            layer_diagnosis['error'] = str(e)
            
        return layer_diagnosis
    
    def _analyze_layer_geometry(self, gdf: gpd.GeoDataFrame) -> Dict[str, Any]:
        """–î–µ—Ç–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ –≥–µ–æ–º–µ—Ç—Ä—ñ—ó —à–∞—Ä—É"""
        
        geom_analysis = {
            'geometry_types': {},
            'validity': {},
            'complexity': {},
            'spatial_distribution': {}
        }
        
        try:
            if 'geometry' not in gdf.columns or gdf.geometry.empty:
                return geom_analysis
            
            # –¢–∏–ø–∏ –≥–µ–æ–º–µ—Ç—Ä—ñ–π
            geom_types = gdf.geometry.geom_type.value_counts()
            geom_analysis['geometry_types'] = {
                'types_distribution': geom_types.to_dict(),
                'primary_type': geom_types.index[0] if len(geom_types) > 0 else None,
                'is_mixed': len(geom_types) > 1
            }
            
            # –í–∞–ª—ñ–¥–Ω—ñ—Å—Ç—å –≥–µ–æ–º–µ—Ç—Ä—ñ–π
            valid_geoms = gdf.geometry.is_valid
            geom_analysis['validity'] = {
                'valid_count': int(valid_geoms.sum()),
                'invalid_count': int((~valid_geoms).sum()),
                'validity_ratio': float(valid_geoms.mean())
            }
            
            # –°–∫–ª–∞–¥–Ω—ñ—Å—Ç—å –≥–µ–æ–º–µ—Ç—Ä—ñ–π (–¥–ª—è –ø–æ–ª—ñ–≥–æ–Ω—ñ–≤ —Ç–∞ –ª—ñ–Ω—ñ–π)
            if geom_types.index[0] in ['Polygon', 'MultiPolygon', 'LineString', 'MultiLineString']:
                complexities = []
                for geom in gdf.geometry.dropna()[:1000]:  # –û–±–º–µ–∂—É—î–º–æ –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
                    if hasattr(geom, 'coords'):
                        complexities.append(len(list(geom.coords)))
                    elif hasattr(geom, 'exterior'):
                        complexities.append(len(list(geom.exterior.coords)))
                
                if complexities:
                    geom_analysis['complexity'] = {
                        'avg_vertices': float(np.mean(complexities)),
                        'max_vertices': int(np.max(complexities)),
                        'min_vertices': int(np.min(complexities))
                    }
            
            # –ü—Ä–æ—Å—Ç–æ—Ä–æ–≤–∏–π —Ä–æ–∑–ø–æ–¥—ñ–ª
            bounds = gdf.total_bounds
            geom_analysis['spatial_distribution'] = {
                'bbox': {
                    'minx': float(bounds[0]), 'miny': float(bounds[1]),
                    'maxx': float(bounds[2]), 'maxy': float(bounds[3])
                },
                'bbox_area_deg2': float((bounds[2] - bounds[0]) * (bounds[3] - bounds[1])),
                'centroid': {
                    'lat': float((bounds[1] + bounds[3]) / 2),
                    'lon': float((bounds[0] + bounds[2]) / 2)
                }
            }
            
        except Exception as e:
            geom_analysis['error'] = str(e)
            
        return geom_analysis
    
    def _analyze_layer_attributes(self, gdf: gpd.GeoDataFrame) -> Dict[str, Any]:
        """–î–µ—Ç–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ —à–∞—Ä—É"""
        
        attr_analysis = {
            'columns_summary': {},
            'data_types': {},
            'completeness': {},
            'uniqueness': {},
            'patterns': {}
        }
        
        try:
            # –ê–Ω–∞–ª—ñ–∑ –∫–æ–∂–Ω–æ—ó –∫–æ–ª–æ–Ω–∫–∏
            for col in gdf.columns:
                if col == 'geometry':
                    continue
                    
                col_info = {
                    'dtype': str(gdf[col].dtype),
                    'null_count': int(gdf[col].isna().sum()),
                    'null_ratio': float(gdf[col].isna().mean()),
                    'unique_count': int(gdf[col].nunique()),
                    'uniqueness_ratio': float(gdf[col].nunique() / len(gdf))
                }
                
                # –î–æ–¥–∞—Ç–∫–æ–≤–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ —Ç–∏–ø—É
                if gdf[col].dtype in ['object', 'string']:
                    # –¢–µ–∫—Å—Ç–æ–≤—ñ –ø–æ–ª—è
                    non_null = gdf[col].dropna()
                    if len(non_null) > 0:
                        col_info.update({
                            'avg_length': float(non_null.astype(str).str.len().mean()),
                            'max_length': int(non_null.astype(str).str.len().max()),
                            'common_values': non_null.value_counts().head(5).to_dict()
                        })
                        
                        # –î–µ—Ç–µ–∫—Ü—ñ—è OSM —Ç–µ–≥—ñ–≤
                        if any(osm_pattern in col.lower() for osm_pattern in 
                               ['osm', 'tag', 'key', 'value', 'amenity', 'shop', 'name']):
                            col_info['likely_osm_tag'] = True
                
                elif gdf[col].dtype in ['int64', 'float64', 'int32', 'float32']:
                    # –ß–∏—Å–ª–æ–≤—ñ –ø–æ–ª—è
                    non_null = gdf[col].dropna()
                    if len(non_null) > 0:
                        col_info.update({
                            'min_value': float(non_null.min()),
                            'max_value': float(non_null.max()),
                            'mean_value': float(non_null.mean()),
                            'median_value': float(non_null.median())
                        })
                
                attr_analysis['columns_summary'][col] = col_info
            
            # –ó–∞–≥–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            non_geom_cols = [col for col in gdf.columns if col != 'geometry']
            attr_analysis['data_types'] = dict(gdf[non_geom_cols].dtypes.astype(str))
            
            attr_analysis['completeness'] = {
                'fully_complete_columns': len([col for col in non_geom_cols 
                                             if gdf[col].notna().all()]),
                'mostly_empty_columns': len([col for col in non_geom_cols 
                                           if gdf[col].isna().mean() > 0.9]),
                'average_completeness': float(gdf[non_geom_cols].notna().mean().mean())
            }
            
        except Exception as e:
            attr_analysis['error'] = str(e)
            
        return attr_analysis
    
    def _analyze_osm_tags_in_layer(self, gdf: gpd.GeoDataFrame) -> Dict[str, Any]:
        """–°–ø–µ—Ü–∏—Ñ—ñ—á–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ OSM —Ç–µ–≥—ñ–≤ —É —à–∞—Ä—ñ"""
        
        osm_analysis = {
            'detected_osm_columns': [],
            'key_osm_tags': {},
            'tag_completeness': {},
            'osm_structure_type': None
        }
        
        try:
            # –ö–ª—é—á–æ–≤—ñ OSM –∫–æ–ª–æ–Ω–∫–∏
            key_osm_columns = [
                'osm_id', 'osm_type', 'osm_version', 'osm_timestamp',
                'amenity', 'shop', 'building', 'landuse', 'highway', 'railway',
                'natural', 'leisure', 'tourism', 'office', 'name', 'addr:housenumber',
                'addr:street', 'addr:city', 'place', 'barrier', 'man_made'
            ]
            
            # –í–∏—è–≤–ª–µ–Ω–Ω—è OSM –∫–æ–ª–æ–Ω–æ–∫
            existing_osm_cols = [col for col in gdf.columns if col in key_osm_columns]
            osm_analysis['detected_osm_columns'] = existing_osm_cols
            
            # –ê–Ω–∞–ª—ñ–∑ –∫–ª—é—á–æ–≤–∏—Ö —Ç–µ–≥—ñ–≤
            for tag in existing_osm_cols:
                if tag in gdf.columns:
                    non_null = gdf[tag].dropna()
                    if len(non_null) > 0:
                        osm_analysis['key_osm_tags'][tag] = {
                            'count': len(non_null),
                            'ratio': float(len(non_null) / len(gdf)),
                            'unique_values': int(non_null.nunique()),
                            'top_values': non_null.value_counts().head(10).to_dict()
                        }
            
            # –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Ç–∏–ø—É OSM —Å—Ç—Ä—É–∫—Ç—É—Ä–∏
            if 'osm_id' in gdf.columns:
                osm_analysis['osm_structure_type'] = 'standard_osm_export'
            elif any(col.startswith('osm_') for col in gdf.columns):
                osm_analysis['osm_structure_type'] = 'processed_osm_data'
            elif any(col in ['amenity', 'shop', 'building'] for col in gdf.columns):
                osm_analysis['osm_structure_type'] = 'osm_themed_export'
            else:
                osm_analysis['osm_structure_type'] = 'unknown_or_non_osm'
            
        except Exception as e:
            osm_analysis['error'] = str(e)
            
        return osm_analysis
    
    def _extract_representative_samples(self, gdf: gpd.GeoDataFrame, 
                                      sample_size: int) -> Dict[str, Any]:
        """–í–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω–∏—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤"""
        
        samples = {
            'random_samples': [],
            'data_rich_samples': [],
            'geometry_samples': {}
        }
        
        try:
            if gdf.empty:
                return samples
            
            # –í–∏–ø–∞–¥–∫–æ–≤—ñ –ø—Ä–∏–∫–ª–∞–¥–∏
            random_sample = gdf.sample(min(sample_size, len(gdf)))
            for idx, row in random_sample.iterrows():
                sample_data = {}
                for col in gdf.columns:
                    if col == 'geometry':
                        if pd.notna(row[col]):
                            sample_data['geometry_type'] = row[col].geom_type
                            sample_data['geometry_valid'] = row[col].is_valid
                    else:
                        if pd.notna(row[col]):
                            sample_data[col] = str(row[col])[:200]  # –û–±—Ä—ñ–∑–∞—î–º–æ –¥–æ–≤–≥—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
                
                samples['random_samples'].append(sample_data)
            
            # –ü—Ä–∏–∫–ª–∞–¥–∏ –∑ –Ω–∞–π–±—ñ–ª—å—à–æ—é –∫—ñ–ª—å–∫—ñ—Å—Ç—é –¥–∞–Ω–∏—Ö
            non_geom_cols = [col for col in gdf.columns if col != 'geometry']
            if non_geom_cols:
                data_richness = gdf[non_geom_cols].notna().sum(axis=1)
                rich_indices = data_richness.nlargest(min(sample_size, len(gdf))).index
                
                for idx in rich_indices:
                    row = gdf.loc[idx]
                    sample_data = {}
                    for col in gdf.columns:
                        if col == 'geometry':
                            if pd.notna(row[col]):
                                sample_data['geometry_type'] = row[col].geom_type
                        else:
                            if pd.notna(row[col]):
                                sample_data[col] = str(row[col])[:200]
                    
                    samples['data_rich_samples'].append(sample_data)
            
            # –ü—Ä–∏–∫–ª–∞–¥–∏ –∑–∞ —Ç–∏–ø–∞–º–∏ –≥–µ–æ–º–µ—Ç—Ä—ñ—ó
            if 'geometry' in gdf.columns:
                geom_types = gdf.geometry.geom_type.value_counts()
                for geom_type in geom_types.index[:5]:  # –¢–æ–ø 5 —Ç–∏–ø—ñ–≤
                    type_examples = gdf[gdf.geometry.geom_type == geom_type].head(2)
                    samples['geometry_samples'][geom_type] = []
                    
                    for idx, row in type_examples.iterrows():
                        geom_sample = {
                            'geometry_type': geom_type,
                            'is_valid': row.geometry.is_valid if pd.notna(row.geometry) else False
                        }
                        
                        # –î–æ–¥–∞—î–º–æ –∫–ª—é—á–æ–≤—ñ –∞—Ç—Ä–∏–±—É—Ç–∏
                        key_attrs = ['name', 'amenity', 'shop', 'building', 'highway']
                        for attr in key_attrs:
                            if attr in gdf.columns and pd.notna(row[attr]):
                                geom_sample[attr] = str(row[attr])
                        
                        samples['geometry_samples'][geom_type].append(geom_sample)
            
        except Exception as e:
            samples['error'] = str(e)
            
        return samples
    
    def _assess_layer_quality(self, gdf: gpd.GeoDataFrame) -> Dict[str, Any]:
        """–û—Ü—ñ–Ω–∫–∞ —è–∫–æ—Å—Ç—ñ –¥–∞–Ω–∏—Ö —É —à–∞—Ä—ñ"""
        
        quality = {
            'overall_score': 0.0,
            'geometry_quality': 0.0,
            'data_completeness': 0.0,
            'osm_richness': 0.0,
            'issues': [],
            'strengths': []
        }
        
        try:
            scores = []
            
            # 1. –Ø–∫—ñ—Å—Ç—å –≥–µ–æ–º–µ—Ç—Ä—ñ—ó
            if 'geometry' in gdf.columns and not gdf.geometry.empty:
                valid_ratio = gdf.geometry.is_valid.mean()
                non_null_ratio = gdf.geometry.notna().mean()
                quality['geometry_quality'] = (valid_ratio + non_null_ratio) / 2
                scores.append(quality['geometry_quality'])
                
                if valid_ratio < 0.95:
                    quality['issues'].append(f"–ù–∏–∑—å–∫–∞ –≤–∞–ª—ñ–¥–Ω—ñ—Å—Ç—å –≥–µ–æ–º–µ—Ç—Ä—ñ–π: {valid_ratio:.2%}")
                else:
                    quality['strengths'].append("–í–∏—Å–æ–∫–æ—è–∫—ñ—Å–Ω—ñ –≥–µ–æ–º–µ—Ç—Ä—ñ—ó")
            
            # 2. –ü–æ–≤–Ω–æ—Ç–∞ –¥–∞–Ω–∏—Ö
            non_geom_cols = [col for col in gdf.columns if col != 'geometry']
            if non_geom_cols:
                completeness = gdf[non_geom_cols].notna().mean().mean()
                quality['data_completeness'] = completeness
                scores.append(completeness)
                
                if completeness < 0.3:
                    quality['issues'].append(f"–ù–∏–∑—å–∫–∞ –ø–æ–≤–Ω–æ—Ç–∞ –¥–∞–Ω–∏—Ö: {completeness:.2%}")
                elif completeness > 0.7:
                    quality['strengths'].append("–í–∏—Å–æ–∫–∏–π —Ä—ñ–≤–µ–Ω—å –∑–∞–ø–æ–≤–Ω–µ–Ω–æ—Å—Ç—ñ –¥–∞–Ω–∏—Ö")
            
            # 3. –ë–∞–≥–∞—Ç—Å—Ç–≤–æ OSM —Ç–µ–≥—ñ–≤
            osm_cols = [col for col in gdf.columns if col in 
                       ['amenity', 'shop', 'building', 'landuse', 'highway', 'name']]
            if osm_cols:
                osm_completeness = gdf[osm_cols].notna().any(axis=1).mean()
                quality['osm_richness'] = osm_completeness
                scores.append(osm_completeness)
                
                if osm_completeness > 0.5:
                    quality['strengths'].append("–ë–∞–≥–∞—Ç—ñ OSM —Ç–µ–≥–∏")
            
            # –ó–∞–≥–∞–ª—å–Ω–∞ –æ—Ü—ñ–Ω–∫–∞
            if scores:
                quality['overall_score'] = np.mean(scores)
                
                if quality['overall_score'] > 0.8:
                    quality['strengths'].append("–í—ñ–¥–º—ñ–Ω–Ω–∞ —è–∫—ñ—Å—Ç—å –¥–∞–Ω–∏—Ö")
                elif quality['overall_score'] < 0.4:
                    quality['issues'].append("–ó–∞–≥–∞–ª–æ–º –Ω–∏–∑—å–∫–∞ —è–∫—ñ—Å—Ç—å –¥–∞–Ω–∏—Ö")
            
        except Exception as e:
            quality['error'] = str(e)
            
        return quality
    
    def _assess_h3_suitability(self, gdf: gpd.GeoDataFrame) -> Dict[str, Any]:
        """–û—Ü—ñ–Ω–∫–∞ –ø—Ä–∏–¥–∞—Ç–Ω–æ—Å—Ç—ñ –¥–ª—è H3 —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó"""
        
        h3_assessment = {
            'suitable_for_h3': False,
            'recommended_resolutions': [],
            'indexing_strategy': None,
            'expected_performance': {},
            'considerations': []
        }
        
        try:
            if 'geometry' not in gdf.columns or gdf.geometry.empty:
                return h3_assessment
            
            geom_types = gdf.geometry.geom_type.value_counts()
            primary_geom = geom_types.index[0] if len(geom_types) > 0 else None
            
            # –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è –ø—Ä–∏–¥–∞—Ç–Ω–æ—Å—Ç—ñ —Ç–∞ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó
            if primary_geom == 'Point':
                h3_assessment['suitable_for_h3'] = True
                h3_assessment['indexing_strategy'] = 'direct_point_to_cell'
                h3_assessment['recommended_resolutions'] = [8, 9, 10]
                h3_assessment['considerations'].append("–Ü–¥–µ–∞–ª—å–Ω–æ –¥–ª—è H3 - –ø—Ä—è–º—ñ —Ç–æ—á–∫–∏")
                
            elif primary_geom in ['Polygon', 'MultiPolygon']:
                h3_assessment['suitable_for_h3'] = True
                h3_assessment['indexing_strategy'] = 'polygon_to_cells_coverage'
                h3_assessment['recommended_resolutions'] = [7, 8, 9]
                h3_assessment['considerations'].append("–ü–æ—Ç—Ä—ñ–±–Ω–µ –ø–æ–∫—Ä–∏—Ç—Ç—è –ø–æ–ª—ñ–≥–æ–Ω—ñ–≤ H3 –∫–æ–º—ñ—Ä–∫–∞–º–∏")
                
            elif primary_geom in ['LineString', 'MultiLineString']:
                h3_assessment['suitable_for_h3'] = True
                h3_assessment['indexing_strategy'] = 'line_intersecting_cells'
                h3_assessment['recommended_resolutions'] = [8, 9]
                h3_assessment['considerations'].append("–Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è —á–µ—Ä–µ–∑ –ø–µ—Ä–µ—Ç–∏–Ω–∞–Ω–Ω—è –∑ H3 –∫–æ–º—ñ—Ä–∫–∞–º–∏")
            
            # –û—Ü—ñ–Ω–∫–∞ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
            feature_count = len(gdf)
            if feature_count < 10000:
                h3_assessment['expected_performance']['indexing_time'] = 'fast'
            elif feature_count < 100000:
                h3_assessment['expected_performance']['indexing_time'] = 'moderate'
            else:
                h3_assessment['expected_performance']['indexing_time'] = 'slow'
                h3_assessment['considerations'].append("–í–µ–ª–∏–∫–∏–π –æ–±—Å—è–≥ –¥–∞–Ω–∏—Ö - –ø–æ—Ç—Ä—ñ–±–Ω–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è")
            
            # –ü—Ä–æ—Å—Ç–æ—Ä–æ–≤–∞ —â—ñ–ª—å–Ω—ñ—Å—Ç—å
            if 'geometry' in gdf.columns:
                bounds = gdf.total_bounds
                area_deg2 = (bounds[2] - bounds[0]) * (bounds[3] - bounds[1])
                density = feature_count / area_deg2 if area_deg2 > 0 else 0
                
                h3_assessment['expected_performance']['spatial_density'] = density
                
                if density > 1000:  # –í–∏—Å–æ–∫–∞ —â—ñ–ª—å–Ω—ñ—Å—Ç—å
                    h3_assessment['considerations'].append("–í–∏—Å–æ–∫–∞ –ø—Ä–æ—Å—Ç–æ—Ä–æ–≤–∞ —â—ñ–ª—å–Ω—ñ—Å—Ç—å - —Ä–æ–∑–≥–ª—è–Ω—É—Ç–∏ –≤–∏—â—ñ —Ä–µ–∑–æ–ª—é—Ü—ñ—ó")
                elif density < 10:  # –ù–∏–∑—å–∫–∞ —â—ñ–ª—å–Ω—ñ—Å—Ç—å
                    h3_assessment['considerations'].append("–ù–∏–∑—å–∫–∞ –ø—Ä–æ—Å—Ç–æ—Ä–æ–≤–∞ —â—ñ–ª—å–Ω—ñ—Å—Ç—å - –º–æ–∂–Ω–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ –Ω–∏–∂—á—ñ —Ä–µ–∑–æ–ª—é—Ü—ñ—ó")
            
        except Exception as e:
            h3_assessment['error'] = str(e)
            
        return h3_assessment
    
    def _analyze_osm_tags_globally(self, layers_data: Dict) -> Dict[str, Any]:
        """–ì–ª–æ–±–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ OSM —Ç–µ–≥—ñ–≤ –ø–æ –≤—Å—ñ—Ö —à–∞—Ä–∞—Ö"""
        
        global_osm = {
            'all_osm_columns': set(),
            'cross_layer_tags': {},
            'tag_distribution': {},
            'semantic_categories': {}
        }
        
        try:
            # –ó–±—ñ—Ä –≤—Å—ñ—Ö OSM –∫–æ–ª–æ–Ω–æ–∫
            for layer_name, layer_data in layers_data.items():
                if 'osm_tags_analysis' in layer_data:
                    osm_cols = layer_data['osm_tags_analysis'].get('detected_osm_columns', [])
                    global_osm['all_osm_columns'].update(osm_cols)
            
            global_osm['all_osm_columns'] = list(global_osm['all_osm_columns'])
            
            # –ê–Ω–∞–ª—ñ–∑ —Ä–æ–∑–ø–æ–¥—ñ–ª—É —Ç–µ–≥—ñ–≤ –ø–æ —à–∞—Ä–∞—Ö
            for tag in global_osm['all_osm_columns']:
                tag_info = {'layers_with_tag': [], 'total_features': 0}
                
                for layer_name, layer_data in layers_data.items():
                    osm_tags = layer_data.get('osm_tags_analysis', {}).get('key_osm_tags', {})
                    if tag in osm_tags:
                        tag_info['layers_with_tag'].append({
                            'layer': layer_name,
                            'count': osm_tags[tag]['count'],
                            'ratio': osm_tags[tag]['ratio']
                        })
                        tag_info['total_features'] += osm_tags[tag]['count']
                
                if tag_info['layers_with_tag']:
                    global_osm['cross_layer_tags'][tag] = tag_info
            
            # –°–µ–º–∞–Ω—Ç–∏—á–Ω—ñ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó
            categories = {
                'retail': ['shop', 'amenity', 'commercial'],
                'transport': ['highway', 'railway', 'public_transport'],
                'buildings': ['building', 'addr:housenumber', 'addr:street'],
                'places': ['place', 'name', 'tourism'],
                'land_use': ['landuse', 'natural', 'leisure']
            }
            
            for category, tags in categories.items():
                category_stats = {'relevant_tags': [], 'total_coverage': 0}
                
                for tag in tags:
                    if tag in global_osm['cross_layer_tags']:
                        category_stats['relevant_tags'].append({
                            'tag': tag,
                            'features': global_osm['cross_layer_tags'][tag]['total_features']
                        })
                        category_stats['total_coverage'] += global_osm['cross_layer_tags'][tag]['total_features']
                
                if category_stats['relevant_tags']:
                    global_osm['semantic_categories'][category] = category_stats
            
        except Exception as e:
            global_osm['error'] = str(e)
            
        return global_osm
    
    def _analyze_geometries_globally(self, layers_data: Dict) -> Dict[str, Any]:
        """–ì–ª–æ–±–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ –≥–µ–æ–º–µ—Ç—Ä—ñ–π"""
        
        geometry_analysis = {
            'total_features_all_layers': 0,
            'geometry_type_distribution': {},
            'spatial_coverage': {},
            'complexity_statistics': {},
            'quality_summary': {}
        }
        
        try:
            # –ó–±—ñ—Ä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –≤—Å—ñ—Ö —à–∞—Ä–∞—Ö
            all_geom_types = Counter()
            total_features = 0
            valid_geometries = 0
            all_bounds = []
            
            for layer_name, layer_data in layers_data.items():
                if 'basic_info' in layer_data:
                    layer_features = layer_data['basic_info'].get('total_features', 0)
                    total_features += layer_features
                    
                    # –ó–±—ñ—Ä —Ç–∏–ø—ñ–≤ –≥–µ–æ–º–µ—Ç—Ä—ñ–π
                    geom_details = layer_data.get('geometry_details', {})
                    geom_types = geom_details.get('geometry_types', {}).get('types_distribution', {})
                    for geom_type, count in geom_types.items():
                        all_geom_types[geom_type] += count
                    
                    # –í–∞–ª—ñ–¥–Ω—ñ—Å—Ç—å
                    validity = geom_details.get('validity', {})
                    valid_geometries += validity.get('valid_count', 0)
                    
                    # Bounds
                    spatial_dist = geom_details.get('spatial_distribution', {})
                    if 'bbox' in spatial_dist:
                        all_bounds.append(spatial_dist['bbox'])
            
            geometry_analysis['total_features_all_layers'] = total_features
            geometry_analysis['geometry_type_distribution'] = dict(all_geom_types)
            
            # –ó–∞–≥–∞–ª—å–Ω–µ –ø–æ–∫—Ä–∏—Ç—Ç—è
            if all_bounds:
                minx = min(bbox['minx'] for bbox in all_bounds)
                miny = min(bbox['miny'] for bbox in all_bounds)
                maxx = max(bbox['maxx'] for bbox in all_bounds)
                maxy = max(bbox['maxy'] for bbox in all_bounds)
                
                geometry_analysis['spatial_coverage'] = {
                    'overall_bbox': {'minx': minx, 'miny': miny, 'maxx': maxx, 'maxy': maxy},
                    'coverage_area_deg2': (maxx - minx) * (maxy - miny),
                    'center_point': {'lat': (miny + maxy) / 2, 'lon': (minx + maxx) / 2}
                }
            
            # –ü—ñ–¥—Å—É–º–æ–∫ —è–∫–æ—Å—Ç—ñ
            if total_features > 0:
                geometry_analysis['quality_summary'] = {
                    'overall_validity_ratio': valid_geometries / total_features,
                    'geometric_diversity': len(all_geom_types),
                    'dominant_geometry_type': all_geom_types.most_common(1)[0][0] if all_geom_types else None
                }
            
        except Exception as e:
            geometry_analysis['error'] = str(e)
            
        return geometry_analysis
    
    def _generate_schema_recommendations(self, file_analysis: Dict) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π –¥–ª—è PostGIS —Å—Ö–µ–º–∏"""
        
        recommendations = {
            'table_structure': {},
            'indexing_strategy': {},
            'h3_integration': {},
            'performance_optimization': {},
            'data_quality_issues': []
        }
        
        try:
            layers_data = file_analysis.get('layers_detailed', {})
            osm_analysis = file_analysis.get('osm_tag_analysis', {})
            geometry_analysis = file_analysis.get('geometry_analysis', {})
            
            # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ç–∞–±–ª–∏—Ü—å
            for layer_name, layer_data in layers_data.items():
                table_name = f"osm_{layer_name}"
                
                basic_info = layer_data.get('basic_info', {})
                geom_details = layer_data.get('geometry_details', {})
                osm_tags = layer_data.get('osm_tags_analysis', {})
                h3_suitability = layer_data.get('h3_suitability', {})
                
                table_rec = {
                    'estimated_rows': basic_info.get('total_features', 0),
                    'primary_geometry_type': geom_details.get('geometry_details', {}).get('primary_type'),
                    'recommended_columns': [],
                    'h3_columns': [],
                    'indexes_needed': []
                }
                
                # –ë–∞–∑–æ–≤—ñ –∫–æ–ª–æ–Ω–∫–∏
                table_rec['recommended_columns'].extend([
                    'id SERIAL PRIMARY KEY',
                    'osm_id BIGINT',
                    f'geom GEOMETRY({table_rec["primary_geometry_type"]}, 4326)' if table_rec["primary_geometry_type"] else 'geom GEOMETRY',
                    'region_name VARCHAR(50)'
                ])
                
                # OSM-—Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏
                detected_osm_cols = osm_tags.get('detected_osm_columns', [])
                for osm_col in detected_osm_cols:
                    if osm_col in ['amenity', 'shop', 'building', 'highway', 'landuse']:
                        table_rec['recommended_columns'].append(f'{osm_col} VARCHAR(100)')
                    elif osm_col == 'name':
                        table_rec['recommended_columns'].append('name VARCHAR(255)')
                    elif osm_col.startswith('addr:'):
                        table_rec['recommended_columns'].append(f'"{osm_col}" VARCHAR(255)')
                
                # H3 –∫–æ–ª–æ–Ω–∫–∏
                if h3_suitability.get('suitable_for_h3', False):
                    for res in [7, 8, 9, 10]:
                        table_rec['h3_columns'].append(f'h3_res_{res} VARCHAR(15)')
                
                # –Ü–Ω–¥–µ–∫—Å–∏
                table_rec['indexes_needed'].extend([
                    f'CREATE INDEX idx_{table_name}_geom ON {table_name} USING GIST (geom)',
                    f'CREATE INDEX idx_{table_name}_osm_id ON {table_name} (osm_id)'
                ])
                
                # H3 —ñ–Ω–¥–µ–∫—Å–∏
                for res in [7, 8, 9, 10]:
                    table_rec['indexes_needed'].append(
                        f'CREATE INDEX idx_{table_name}_h3_res_{res} ON {table_name} (h3_res_{res})'
                    )
                
                # –ö–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω—ñ —ñ–Ω–¥–µ–∫—Å–∏
                for cat_col in ['amenity', 'shop', 'building', 'highway']:
                    if cat_col in detected_osm_cols:
                        table_rec['indexes_needed'].append(
                            f'CREATE INDEX idx_{table_name}_{cat_col} ON {table_name} ({cat_col})'
                        )
                
                recommendations['table_structure'][table_name] = table_rec
            
            # –ó–∞–≥–∞–ª—å–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—è —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó
            total_features = geometry_analysis.get('total_features_all_layers', 0)
            
            recommendations['indexing_strategy'] = {
                'h3_resolutions': [7, 8, 9, 10],
                'spatial_index_type': 'GIST',
                'partitioning_needed': total_features > 1000000,
                'materialized_views_recommended': total_features > 500000,
                'composite_indexes': [
                    '(region_name, h3_res_8)',
                    '(amenity, h3_res_9)',
                    '(shop, h3_res_9)'
                ]
            }
            
            # H3 —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è
            recommendations['h3_integration'] = {
                'precompute_h3_cells': True,
                'h3_functions_needed': [
                    'h3_geo_to_h3(lat, lon, resolution)',
                    'h3_h3_to_children(h3_index, resolution)',
                    'h3_h3_to_parent(h3_index, resolution)',
                    'h3_k_ring(h3_index, k)'
                ],
                'h3_utility_tables': [
                    'h3_ukraine_grid_res_7',
                    'h3_ukraine_grid_res_8', 
                    'h3_ukraine_grid_res_9',
                    'h3_ukraine_grid_res_10'
                ]
            }
            
            # –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
            recommendations['performance_optimization'] = {
                'connection_pooling': True,
                'batch_insert_size': 10000,
                'vacuum_schedule': 'weekly',
                'analyze_schedule': 'daily',
                'cluster_on_h3': True if total_features > 100000 else False
            }
            
            # –ü—Ä–æ–±–ª–µ–º–∏ —è–∫–æ—Å—Ç—ñ –¥–∞–Ω–∏—Ö
            for layer_name, layer_data in layers_data.items():
                quality = layer_data.get('data_quality', {})
                issues = quality.get('issues', [])
                if issues:
                    recommendations['data_quality_issues'].extend([
                        f"–®–∞—Ä {layer_name}: {issue}" for issue in issues
                    ])
            
        except Exception as e:
            recommendations['error'] = str(e)
            
        return recommendations
    
    def run_comprehensive_diagnosis(self, target_file: str = None) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ –ø–æ–≤–Ω–æ—ó –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏"""
        
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–æ–≤–Ω–æ—ó –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ HOT OSM –µ–∫—Å–ø–æ—Ä—Ç—ñ–≤")
        
        # –ó–Ω–∞—Ö–æ–¥–∏–º–æ —Ñ–∞–π–ª–∏
        gpkg_files = list(self.data_directory.glob("*.gpkg"))
        
        if not gpkg_files:
            logger.error(f"‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ .gpkg —Ñ–∞–π–ª—ñ–≤ –≤ {self.data_directory}")
            return {}
        
        # –í–∏–±–∏—Ä–∞—î–º–æ —Ñ–∞–π–ª –¥–ª—è –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
        if target_file:
            target_path = self.data_directory / target_file
            if not target_path.exists():
                logger.error(f"‚ùå –§–∞–π–ª {target_file} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
                return {}
            files_to_analyze = [target_path]
        else:
            # –í–∏–±–∏—Ä–∞—î–º–æ –Ω–∞–π–º–µ–Ω—à–∏–π —Ñ–∞–π–ª –¥–ª—è —à–≤–∏–¥–∫–æ—ó –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
            files_to_analyze = sorted(gpkg_files, key=lambda f: f.stat().st_size)[:1]
            logger.info(f"üìã –û–±—Ä–∞–Ω–æ –¥–ª—è –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏: {files_to_analyze[0].name}")
        
        # –ó–∞–ø—É—Å–∫ –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
        diagnosis_results = {}
        
        for gpkg_file in files_to_analyze:
            file_diagnosis = self.diagnose_single_file(gpkg_file)
            diagnosis_results[gpkg_file.name] = file_diagnosis
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø—ñ–¥—Å—É–º–∫–æ–≤–æ–≥–æ –∑–≤—ñ—Ç—É
        summary_report = self._create_diagnosis_summary(diagnosis_results)
        
        complete_diagnosis = {
            'diagnosis_timestamp': datetime.now().isoformat(),
            'analyzed_files': list(diagnosis_results.keys()),
            'file_details': diagnosis_results,
            'summary_report': summary_report,
            'next_steps': self._generate_next_steps(summary_report)
        }
        
        return complete_diagnosis
    
    def _create_diagnosis_summary(self, diagnosis_results: Dict) -> Dict[str, Any]:
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø—ñ–¥—Å—É–º–∫–æ–≤–æ–≥–æ –∑–≤—ñ—Ç—É –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏"""
        
        summary = {
            'files_analyzed': len(diagnosis_results),
            'total_layers_found': 0,
            'total_features': 0,
            'common_layer_structure': {},
            'osm_data_richness': {},
            'geometry_overview': {},
            'schema_complexity': {},
            'key_findings': []
        }
        
        try:
            all_layers = set()
            all_osm_tags = set()
            all_geom_types = Counter()
            
            for file_name, file_data in diagnosis_results.items():
                layers_detailed = file_data.get('layers_detailed', {})
                summary['total_layers_found'] += len(layers_detailed)
                
                for layer_name, layer_data in layers_detailed.items():
                    all_layers.add(layer_name)
                    
                    # –ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ –æ–±'—î–∫—Ç—ñ–≤
                    basic_info = layer_data.get('basic_info', {})
                    summary['total_features'] += basic_info.get('total_features', 0)
                    
                    # OSM —Ç–µ–≥–∏
                    osm_analysis = layer_data.get('osm_tags_analysis', {})
                    detected_cols = osm_analysis.get('detected_osm_columns', [])
                    all_osm_tags.update(detected_cols)
                    
                    # –ì–µ–æ–º–µ—Ç—Ä—ñ—ó
                    geom_details = layer_data.get('geometry_details', {})
                    geom_types = geom_details.get('geometry_types', {}).get('types_distribution', {})
                    for geom_type, count in geom_types.items():
                        all_geom_types[geom_type] += count
            
            summary['common_layer_structure'] = {
                'unique_layers': list(all_layers),
                'layer_count': len(all_layers)
            }
            
            summary['osm_data_richness'] = {
                'unique_osm_tags': list(all_osm_tags),
                'osm_tag_count': len(all_osm_tags),
                'key_retail_tags': [tag for tag in all_osm_tags 
                                  if tag in ['amenity', 'shop', 'building', 'name']]
            }
            
            summary['geometry_overview'] = {
                'geometry_types_found': dict(all_geom_types),
                'dominant_geometry': all_geom_types.most_common(1)[0][0] if all_geom_types else None
            }
            
            # –ö–ª—é—á–æ–≤—ñ –∑–Ω–∞—Ö—ñ–¥–∫–∏
            if len(all_layers) <= 3:
                summary['key_findings'].append(f"–ü—Ä–æ—Å—Ç–∏—á–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞: {len(all_layers)} —à–∞—Ä—ñ–≤")
            else:
                summary['key_findings'].append(f"–°–∫–ª–∞–¥–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞: {len(all_layers)} —à–∞—Ä—ñ–≤")
            
            if 'amenity' in all_osm_tags and 'shop' in all_osm_tags:
                summary['key_findings'].append("–í—ñ–¥–º—ñ–Ω–Ω–æ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å –¥–ª—è —Ä–µ—Ç–µ–π–ª –∞–Ω–∞–ª—ñ–∑—É")
            
            if summary['total_features'] > 100000:
                summary['key_findings'].append("–í–µ–ª–∏–∫–∏–π –æ–±—Å—è–≥ –¥–∞–Ω–∏—Ö - –ø–æ—Ç—Ä—ñ–±–Ω–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è")
            
        except Exception as e:
            summary['error'] = str(e)
            
        return summary
    
    def _generate_next_steps(self, summary: Dict) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –Ω–∞—Å—Ç—É–ø–Ω–∏—Ö –∫—Ä–æ–∫—ñ–≤"""
        
        next_steps = []
        
        try:
            total_features = summary.get('total_features', 0)
            osm_richness = summary.get('osm_data_richness', {})
            
            # –ë–∞–∑–æ–≤—ñ –∫—Ä–æ–∫–∏
            next_steps.append("1. –°—Ç–≤–æ—Ä–∏—Ç–∏ PostGIS —Å—Ö–µ–º—É –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π")
            next_steps.append("2. –†–æ–∑—Ä–æ–±–∏—Ç–∏ ETL –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è —ñ–º–ø–æ—Ä—Ç—É OSM –¥–∞–Ω–∏—Ö")
            
            # –£–º–æ–≤–Ω—ñ –∫—Ä–æ–∫–∏
            if total_features > 500000:
                next_steps.append("3. –ù–∞–ª–∞—à—Ç—É–≤–∞—Ç–∏ –ø–∞—Ä—Ç–∏—Ü—ñ–æ–Ω—É–≤–∞–Ω–Ω—è —Ç–∞–±–ª–∏—Ü—å –∑–∞ —Ä–µ–≥—ñ–æ–Ω–∞–º–∏")
                next_steps.append("4. –°—Ç–≤–æ—Ä–∏—Ç–∏ –º–∞—Ç–µ—Ä—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—è –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü—ñ–π")
            
            if len(osm_richness.get('key_retail_tags', [])) >= 3:
                next_steps.append("3. –°—Ç–≤–æ—Ä–∏—Ç–∏ —Å–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ —ñ–Ω–¥–µ–∫—Å–∏ –¥–ª—è —Ä–µ—Ç–µ–π–ª –∞–Ω–∞–ª—ñ–∑—É")
            
            next_steps.append("5. –Ü–Ω—Ç–µ–≥—Ä—É–≤–∞—Ç–∏ H3 —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—é –¥–ª—è –≤—Å—ñ—Ö —Ä–µ–∑–æ–ª—é—Ü—ñ–π (7-10)")
            next_steps.append("6. –ü—Ä–æ—Ç–µ—Å—Ç—É–≤–∞—Ç–∏ —ñ–º–ø–æ—Ä—Ç –Ω–∞ –æ–¥–Ω—ñ–π –æ–±–ª–∞—Å—Ç—ñ")
            next_steps.append("7. –ú–∞—Å—à—Ç–∞–±—É–≤–∞—Ç–∏ –Ω–∞ –≤—Å—ñ –æ–±–ª–∞—Å—Ç—ñ –£–∫—Ä–∞—ó–Ω–∏")
            
        except Exception as e:
            next_steps.append(f"–ü–æ–º–∏–ª–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –∫—Ä–æ–∫—ñ–≤: {e}")
            
        return next_steps
    
    def save_diagnosis_report(self, diagnosis_results: Dict, output_path: str = None):
        """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∑–≤—ñ—Ç—É –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏"""
        
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"hot_osm_diagnosis_{timestamp}.json"
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(diagnosis_results, f, ensure_ascii=False, indent=2, default=str)
            
            logger.info(f"‚úÖ –ó–≤—ñ—Ç –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {output_path}")
            
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∑–≤—ñ—Ç—É: {e}")
    
    def print_diagnosis_summary(self, diagnosis_results: Dict):
        """–í–∏–≤–µ–¥–µ–Ω–Ω—è –∫–æ—Ä–æ—Ç–∫–æ–≥–æ –∑–≤—ñ—Ç—É –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏"""
        
        print("\n" + "="*80)
        print("üîç –î–Ü–ê–ì–ù–û–°–¢–ò–ö–ê HOT OSM –ï–ö–°–ü–û–†–¢–Ü–í - –ü–Ü–î–°–£–ú–û–ö")
        print("="*80)
        
        summary = diagnosis_results.get('summary_report', {})
        file_details = diagnosis_results.get('file_details', {})
        
        print(f"üìÖ –ß–∞—Å –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏: {diagnosis_results.get('diagnosis_timestamp', 'N/A')}")
        print(f"üìÅ –ü—Ä–æ–∞–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω–æ —Ñ–∞–π–ª—ñ–≤: {summary.get('files_analyzed', 0)}")
        print(f"üìä –ó–Ω–∞–π–¥–µ–Ω–æ —à–∞—Ä—ñ–≤: {summary.get('total_layers_found', 0)}")
        print(f"üî¢ –ó–∞–≥–∞–ª–æ–º –æ–±'—î–∫—Ç—ñ–≤: {summary.get('total_features', 0):,}")
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —à–∞—Ä—ñ–≤
        print(f"\nüìã –°–¢–†–£–ö–¢–£–†–ê –®–ê–†–Ü–í:")
        layer_structure = summary.get('common_layer_structure', {})
        unique_layers = layer_structure.get('unique_layers', [])
        for layer in unique_layers:
            print(f"  ‚Ä¢ {layer}")
        
        # OSM –±–∞–≥–∞—Ç—Å—Ç–≤–æ
        print(f"\nüè∑Ô∏è OSM –¢–ï–ì–ò:")
        osm_richness = summary.get('osm_data_richness', {})
        osm_tags = osm_richness.get('unique_osm_tags', [])
        retail_tags = osm_richness.get('key_retail_tags', [])
        
        print(f"  –í—Å—å–æ–≥–æ —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö —Ç–µ–≥—ñ–≤: {len(osm_tags)}")
        print(f"  –ö–ª—é—á–æ–≤—ñ —Ä–µ—Ç–µ–π–ª —Ç–µ–≥–∏: {', '.join(retail_tags)}")
        
        # –ì–µ–æ–º–µ—Ç—Ä—ñ—ó
        print(f"\nüìê –ì–ï–û–ú–ï–¢–†–Ü–á:")
        geom_overview = summary.get('geometry_overview', {})
        geom_types = geom_overview.get('geometry_types_found', {})
        for geom_type, count in geom_types.items():
            print(f"  ‚Ä¢ {geom_type}: {count:,} –æ–±'—î–∫—Ç—ñ–≤")
        
        # –ö–ª—é—á–æ–≤—ñ –∑–Ω–∞—Ö—ñ–¥–∫–∏
        print(f"\nüí° –ö–õ–Æ–ß–û–í–Ü –ó–ù–ê–•–Ü–î–ö–ò:")
        key_findings = summary.get('key_findings', [])
        for finding in key_findings:
            print(f"  ‚úì {finding}")
        
        # –î–µ—Ç–∞–ª—å–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø–æ —Ñ–∞–π–ª–∞—Ö
        print(f"\nüìÑ –î–ï–¢–ê–õ–Ü –ü–û –§–ê–ô–õ–ê–•:")
        for file_name, file_data in file_details.items():
            file_info = file_data.get('file_info', {})
            sqlite_structure = file_data.get('sqlite_structure', {})
            
            print(f"\n  üìÅ {file_name}")
            print(f"    –†–æ–∑–º—ñ—Ä: {file_info.get('size_mb', 0):.1f} MB")
            
            tables = sqlite_structure.get('tables', [])
            print(f"    SQLite —Ç–∞–±–ª–∏—Ü—ñ ({len(tables)}):")
            for table in tables[:5]:  # –ü–æ–∫–∞–∑—É—î–º–æ –ø–µ—Ä—à—ñ 5
                print(f"      ‚Ä¢ {table.get('name', 'N/A')}: {table.get('row_count', 0):,} –∑–∞–ø–∏—Å—ñ–≤")
            
            if len(tables) > 5:
                print(f"      ... —Ç–∞ —â–µ {len(tables) - 5} —Ç–∞–±–ª–∏—Ü—å")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó
        print(f"\nüéØ –ù–ê–°–¢–£–ü–ù–Ü –ö–†–û–ö–ò:")
        next_steps = diagnosis_results.get('next_steps', [])
        for step in next_steps:
            print(f"  {step}")
        
        print(f"\n{'='*80}")


def main():
    """–ó–∞–ø—É—Å–∫ –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ HOT OSM –µ–∫—Å–ø–æ—Ä—Ç—ñ–≤"""
    
    data_directory = r"C:\OSMData"
    
    print("üîç –ó–∞–ø—É—Å–∫ –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ HOT OSM –µ–∫—Å–ø–æ—Ä—Ç—ñ–≤...")
    print(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è: {data_directory}")
    
    analyzer = HOTOSMDiagnosticAnalyzer(data_directory)
    
    try:
        # –ó–∞–ø—É—Å–∫ –ø–æ–≤–Ω–æ—ó –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
        diagnosis_results = analyzer.run_comprehensive_diagnosis()
        
        if diagnosis_results:
            # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∑–≤—ñ—Ç—É
            analyzer.save_diagnosis_report(diagnosis_results)
            
            # –í–∏–≤–µ–¥–µ–Ω–Ω—è –ø—ñ–¥—Å—É–º–∫—É
            analyzer.print_diagnosis_summary(diagnosis_results)
            
            print(f"\n‚úÖ –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø—ñ—à–Ω–æ!")
        else:
            print(f"‚ùå –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –Ω–µ –¥–∞–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤")
            
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏: {e}")
        logger.exception("–î–µ—Ç–∞–ª—å–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –ø–æ–º–∏–ª–∫—É:")


if __name__ == "__main__":
    main()